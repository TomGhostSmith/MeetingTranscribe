好的,那个谁讲,可以听得听吗?	20.72
可以的,可以的。	30.18
那个是谁讲吗,诗涛?	70.82
是我讲,我讲那个论文,Metagraph。	74.3
我共享一下屏幕。	79.7
OK,我来讲一下这篇这个Metagraph,	91.49
然后Metagraph它其实就是,	97.31
我觉得跟上次学的那个Lexi,	100.77
就上次我看的那篇LexiMap还是有一点相像之处的,	103.81
但是我感觉比LexiMap好像更加的robust一点。	107.41
那么它是一个在大规模生物序列库中实现高效并且准确的这个全文本搜索的一个框架。	111.41
然后以下是这篇文章的作者,	119.01
然后基本上都是从ETH这个学校的。	123.11
然后他所遇到的这个主要的挑战首先是,	130.41
这个数据库的规模报道性的增长。	133.71
然后第二个就是它有一个全文本搜索的不可行性,	135.91
也就是长期以来这个,	139.91
就是Petabase,就是Pet字节级别的这个原始测试数据是一直没有办法进行全文本搜索的。	142.01
那它这里的全文本搜索的意思是检索所有包含与给定的查询序列相似序列的数据集。	150.41
那么这篇文章就为了解决,	157.71
就需要解决这个框架。	161.31
就需要解决这个问题。	163.71
那么还有一些现有的工具。	164.71
现有的工具有比如说这个草图技术。	168.61
那么这个草图技术呢,	172.31
就是基于这个哈希算法去生成这个数据的一个比较紧凑的摘要。	173.81
然后这个摘要去用来去做近似级的相似性和包含的查询。	179.61
但是他提的一个例子就是叫Pebble Scout这个工具。	185.61
然后虽然说他觉得这个扩展性很强,	190.41
但是他会缺乏比较敏感的比对支持,	193.21
并且存在比较高的假阳性率。	197.31
这个是草图技术,草图技术局限。	199.81
另外一个叫这个Approximate Membership Query Data Structure。	202.71
然后他的代表的一个技术是Bloom Filters。	207.41
那么他主要是用于对序列集合进行单个短序列的近似查询。	212.61
但是这边提到的比如说COBS方法和KM index,	218.31
他也存在有比较强的这种假阳性的匹配问题。	223.01
也就是他们会错误的报告一个序列是存在于数据集当中的。	227.81
然后这样的话就不太好。	232.61
第三个就是精确表示的带注式的Debruin图。	236.21
那么这种做法是可以保证精确性,	240.61
但是它在可扩展性和查询功能多样性之间难以保持平衡。	243.51
那么现有的这个带注式Debruin图的查询功能多样性比较少。	247.71
也就是,	252.21
很可能只能支持比如说KM的精确匹配。	252.81
所以说这篇论文就是基于这个精确表示的带注式Debruin图的这个想法,	257.01
去进一步深挖得到的一个比较完整的框架。	264.41
那么Metagraph其实它的核心想法,	271.38
也就是说它其实主要做的事情就是去构建了一个索引。	273.78
那么这个索引它是两块东西组成的。	277.98
那么第一块就是一个叫KM字典的东西。	280.58
那么这个KM字典,	283.98
其实本质上就是一个Debruin图。	285.08
可以看看右边,	287.88
这个这块,	289.88
右上角这块,	290.78
其实就是他们构建的一个Debruin Graph。	291.88
这个Debruin Graph具体是什么东西呢?	294.58
就是它会存,	296.68
它会根据KM的信息,	298.78
把这个不同的,	302.18
把这个存在K典一个重合的这个KM用一根有象的线段连起来。	304.08
那么这样的话,	312.88
整个一个,	314.08
整个一个数据库就会被以一种图的形式存下来。	314.88
然后存的过程中,	320.28
其实这个有象的线段是不需要存,	322.08
它其实只要存,	324.68
就比如说这个这个TCG,	325.68
CCT和GTA就可以了,	328.28
它并不需要存中间这个连接的这根线段,	330.28
因为通过存储的这个set就可以还原出来整个Graph的一个样子。	334.28
然后这是KM字典,	340.48
那么它既然还需要,	342.58
就是一些别的信息,	344.68
就比如说你给它一个query查询的一个段,	347.18
然后你需要知道它,	351.08
比如说这个,	352.48
这个这一段基因是从哪里来的,	353.68
以及这段基因可能拥有的其他信息,	356.58
包括但不限于这个分类信息什么的,	358.78
所以它还需要一个注视,	360.88
它还需要一层注视,	363.48
那么为了这个需求,	365.28
它就是构建了一个叫注视矩阵的东西,	367.08
和这个上面提到这个Debruing Graphs联合在一起,	370.08
就可以,	373.38
就理论上可以完成这个序列的查询工作。	374.68
那么接下来可以继续想一想的Kimmer Set,	380.18
这个Kimmer Set它这个存储的方式,	384.08
它这边提到了三种,	388.58
那么首先是一个叫Hash DBG的东西,	390.18
这个Hash DBG呢,	393.18
就是主要用于内部的操作,	394.88
比如说P处理的序列查找,	396.88
然后第二个叫Bitmap DBG,	399.38
就是它把Kimmer,	401.78
因为Kimmer的这个可能的Kimmer其实数量是有限的,	403.18
那么它就把它存储成一个二进制向量,	407.58
就表示哪一些Kimmer存在于这个集合当中。	410.28
那么第三个东西就是Succinct DBG,	414.88
那么它是基于一个东西叫BOSS表构写的,	418.58
这BOSS其实是四个人的首字母加在一起,	421.78
然后这其实是类似于一个就是点七,	426.48
就是类似点七的,	430.98
或者点ZIP的一种压缩文件格式,	432.28
但是是专门为了这种DNA sequence,	434.68
RNA sequence去做的,	437.48
那么使用这种构件呢,	439.98
会表现出最佳的压缩性能,	442.08
那么这也是作为默认的一种压缩表示的,	443.98
用来减少这个索引的大小,	446.78
然后接下来就说一说这个,	452.42
去具体说一说这个注释矩阵它是怎么做的,	454.72
那么它的注释矩阵其实就是一个N乘M的一个,	459.02
N乘M的一个01,	463.12
那么N乘M的01比如说长这样,	464.62
然后我们看比如说它DI行,	468.52
DJ列,	474.06
如果DI行DJ列这个东西是1呢,	476.36
那说明了就是这个从列来看是这个K门,	480.51
从行来看呢就是这个数,	488.21
就是D这个属性,	490.31
那么它的意思就是DI个K门会具有D这个属性,	492.11
它是用D,	496.81
用这样的方式来,	497.71
然后它当然旁边可能还会有张表,	499.61
就比如说DJ一个属性,	501.81
那么具体的属性是什么,	505.51
那么它通过这一张I乘J的这张表01表,	509.82
其实就表明了比如说具体的哪个K门,	514.12
哪些K门会具有J的这个属性,	520.42
那么从这个图上其实大概可以看出来它里面会存在很多0,	523.22
所以说这个矩阵其实是,	527.72
规模比较庞大,	529.92
但通常极其稀疏的,	530.62
那么论文中也提到它的这个列数一般有10的12次方,	531.82
然后它的行数大概有10的9次方附近,	535.32
所以它确实很庞大,	538.42
但是正是因为它是个稀疏矩阵,	539.62
所以可以说比较高效的压缩,	541.42
这个压缩基本上就是选取就是专门的用来存稀疏矩阵这个存法,	543.42
这个也比较直观吧,	550.12
然后接下来就,	556.68
之前其实我们讲了这个,	562.49
这个K文这个set以及入式矩阵的这个定义吧,	564.59
那么接下来我们就看一下它这个具体是怎么去构建这两个东西,	570.59
那么要构建这两个东西呢,	575.59
它首先做的第一步是去做这个叫数据预处理,	577.59
这个数据预处理呢,	582.59
其实就是从原始输入样本中去构建单图图的这个debrun图,	583.59
那么也就是如右图中所示的,	588.79
比如说从这个男的女的,	590.79
还有动物,	593.49
植物,	594.39
这个不同的样本上面去构建单独的debrun图,	595.19
就是这张小图,	598.89
那么它可选的一步步骤是对,	600.39
它可选但推进的一步步骤是对每一个样本图去进行一次叫图清洗的,	604.39
就是图片上边写的这个optional graph cleaning,	609.39
叫它用的是pop bubbles和prune tips的方法,	613.29
那么那说人话的就是它基于这个cammer分度阈值去识别和修剪这个虚假的路径,	616.89
也就是比如说如果哪一个cammer它的这个风度比较低的话,	623.39
它就会把它,	629.39
它就会把prune tips把它删掉,	630.39
那么清洗它是去,	633.39
这个清洗决定基于cammer所属的unitake中的这个中位度分数,	635.39
那么低风度的这个cammer如果位于只有足够多高风度cammer的unitake中,	640.39
它还是会被保证,	646.39
否则它们会被丢弃,	647.39
然后第三个就是它会把短余给定阈值,	649.39
它通常写的是2000个这个检测队的这个末端tips给修剪掉,	652.39
那么清洗后的这个样本图就会被分解成一系列的这个comptics或者unitics,	657.39
那么这些序列会用于后续去构建这个联合图,	664.39
也就是中间这块东西,	668.39
好,	672.39
那么数据集我们先预处理过了,	675.85
然后接下来就开始构建这个大图了,	678.85
那么其实这一步也没什么好说,	682.85
就是把所有的在第一阶段获得的这个comptics或者unitics去合并,	684.85
去形成一个单一的这个叫joint de Bruin graph,	688.85
那么它在这一步操作里面其实会做一些优化,	692.85
比如说当它所引来自未知链的原始读取的时候,	696.85
这个metagraph会构建一个叫canonical graph,	700.85
然后这个graph当中的每一个cammer以及它的反向互补序列都会被包含在里面,	703.85
那么为了提高压缩效率,	709.85
它的构建主图的这个机制里面是这样的,	711.85
它只储存每对cammer及其反向互补序列中的一个方向,	714.85
然后这个影视表示的另一个方向其实就是不表示,	720.85
那么这样的话它就可以在简洁的表示,	724.85
比如说就是基于之前所说的boss表,	729.85
这种简洁表示中去实现高效的压缩,	731.85
去避免添加额外的虚拟cammer,	734.85
然后构建完了de Bruin的那个graph,	736.85
之后我们就要去构建那个之前所说的灵异矩阵,	745.35
但是这里插一嘴,	748.35
其实它这个不一定是灵异矩阵,	749.35
它那个也可以是有值的矩阵去包含更多的信息,	751.35
这是在论文之后的地方所说的,	755.35
那么在这个de Bruin graph构建完成之后,	758.35
它可以去做并行的叠,	762.35
它可以并行的叠在不同的样本,	764.35
并且把每个样本的所有cammer映射到联合图上去生成单个的注视列,	766.35
那么注视矩阵随后会被压缩成最适合目标应用的表示形式,	773.35
也就是之前所说的要去表示一个系数矩阵,	777.35
那么首先是它采用一个叫row difference的压缩技术,	781.35
那么这个压缩技术其实主要考虑的一点是,	786.35
这个de Bruin graph当中相邻的节点,	789.35
它的注视其实也应该是相似的,	792.35
这也是比较符合知觉的一点,	794.35
就是你毕竟这个剪辑差不太多,	796.35
然后它所对应的注视其实也差不太多,	799.35
所以说它接下来作为一部信息的压缩工作,	803.35
其实是把节点的原始注视计划成与它后继的节点对应的注视之间的相似程度,	806.35
那么通过把原始的信息改成相对差异,	814.35
那么它可以节省很多注视和注视之间重叠的部分,	821.35
这样的话整个注视矩阵可能从本来的1比较多,	826.35
变成1比较少,	831.35
它只存了一个比如说差异量,	832.35
那么这样的话可以把它的压缩性能进步的提高,	835.35
那么接下来就是它的矩阵表示方案了,	840.35
那么它这边就提了比如说Row Spars,	843.35
Column Compressed和Multi-BRWT这三种注视矩阵表示方式,	845.35
那么它的推荐其实就是在稀疏化之后,	853.35
列转换为Multi-BRWT或者是Row Spars表示,	856.35
然后这样的话可以平衡速度和内存需求,	861.35
另外一种Column Compressed它对内存需求比较高,	864.35
那么之前我们讲了这个整个索引的构建过程,	868.35
那么接下来讲讲它的查询,	873.85
不好意思我喝过水,	875.85
OK,	878.85
在这个Metagraph它的查询操作,	885.81
就是之前说,	887.81
就是之前有说别的工具可能只支持这个精确CAMERA匹配,	889.81
那么它做的好的地方就是在于,	893.81
首先是它精确CAMERA匹配,	895.81
一定是可以的,	896.81
那么它里面牵涉了一些优化的技术,	897.81
那么第二个就是它可以做序列到图匹配,	901.81
那么这个东西其实是针对变异性比较高的数据,	906.81
也就是它并不是一个完全精确的匹配,	909.81
它就是匹配这个与这个给定的这个索引库里面,	913.81
匹配最长的这个路径,	918.81
然后它用的方式就是取一个Seed,	921.81
然后Extend,	923.81
这有点像上次提到的那个LexiMap,	924.81
第三个就是它可以去做这个Batched Sequence Search,	928.81
就是做P量化的这个查询算法,	933.81
然后它也对P量化的查询算法做了一些优化,	937.81
接下来就是先讲一下这个精确CAMERA匹配,	941.81
那么精确CAMERA匹配里面它所用到的东西,	945.81
其实就是当映射CAMERA到主图的时候,	949.81
如果查询的CAMERA没有找到,	952.81
那么就很可能是它的这个反向互补CAMERA存在的,	953.81
那么它会去优化这个查询顺序,	958.81
并且对之后的查询使用这个反向互补序列去进行查询,	961.81
那么第二个就是它这个图如果是在Boss表当中去进行表示的时候,	967.81
它去做一步操作教学,	972.81
它会去索引这个CAMERA在Boss表中的范围,	974.81
这样的话可以加快CAMERA的这个查找速度,	977.81
然后接下来就是它,	983.22
这个重头戏,	985.22
就是序列到图的匹配,	987.22
那么这个序列到图的匹配它识别图中最接近的匹配路径的比对方法,	989.22
是用Seed and Extend的方法,	993.22
那么它其实就是先找一个Seed出来,	995.22
然后这个Seed的种子就是Unity里面连续的CAMERA匹配组成的,	998.22
那么第二个就是Extend,	1004.22
那么它每个种子就会在图内向前和向后扩展,	1005.22
用来生成就是比较完整的一个局部的比对,	1009.22
然后它其实这边主要用的想法就是,	1012.22
这个DP跟上次的Extend比较像,	1014.22
然后比对方式的话它会提供三种比对方式,	1016.22
那么我们接下来来比较详细的讲一下这个Seed and Extend的方法,	1019.22
那么找种子的话,	1023.22
如果长的种子就没什么好处,	1026.22
就找着设,	1028.22
但是它说,	1029.22
它这个比较特殊的是,	1030.22
它如果寻查,	1031.22
它如果查找这个短于K的种子,	1033.22
它会执行这个三步,	1036.22
那么第一步是它会去搜索查询这个,	1038.22
就是当这个K比较小于K的时候,	1041.22
K一般取的是这个长度是19,	1043.22
那么在小于19的情况下,	1045.22
它搜索去查询这个是正向KMERS,	1047.22
然后之后它会去反向互补去找一下,	1050.22
最后它通过图的便利去确保找到这个后缀的匹配,	1053.22
那么这个就是它针对这个短的种子去进行的一些优化,	1057.22
接下来是扩展阶段,	1063.22
这个比较重要,	1065.22
就是它的扩展阶段首先是定义了三个分属向量,	1066.22
一个是S一个F,	1070.22
然后S这三个向量的长度都等于查询的长度L,	1072.22
那么比如说你针对一个具体的I而言,	1078.22
SI就是存储的查询前缀S1到SI,	1081.22
也就是结束到节点B的最佳比对分数,	1085.22
然后这个EI和FI就分别代表以插入和删除,	1089.22
在结束于节点B的最佳比对分数,	1097.98
这个其实就已经,	1100.98
这个建模其实已经很像这个GP的这种格式了,	1102.98
那么接下来它其实做的事情就是去,	1108.98
它其实就是从种子的起始界点,	1112.98
比如VS开始,	1115.98
它会去构建一颗比对数,	1116.98
那么这个比对数会编码所有的这个从VS开始的这个所有路径,	1118.98
然后另外这个比对数TS它这个size可能会很大,	1125.98
一种最坏的情况就是这个图是循环的,	1131.98
那么这个TS可能是无限大,	1134.98
它这个数可以不停的往下延伸,	1136.98
那么接下来它就说,	1139.98
我既然这个大小无限大,	1142.98
我计算资源不能无限大,	1145.98
所以它要引入一些这个heuristic,	1147.98
一些启发式的限制,	1149.98
启发式的限制它提了一下三个准则,	1151.98
那么首先是比如说一个元素的分数低于当前计算的就是最佳比对分数,	1155.98
然后如果低了超过X了,	1160.98
那么这个元素就会被跳过,	1163.98
第二个准则就是对于图节点的这个聚合分数,	1165.98
它叫aggregate scores,	1171.98
那么如果你遍离这个节点,	1176.98
但是这个聚合分数没有被更新,	1179.98
那么它也会被从这个VS那个数里面去丢弃掉,	1181.98
那么第三个就是节点探索限制,	1186.98
也就是对于这个数,	1188.98
它这个总深度会有一个怎么说呢,	1189.98
它会有个限制,	1192.98
所以说这三个准则加在一起就确保了,	1194.98
它可以在这个比较短的时间里面去完成整个数的匹配,	1198.98
当然它丢失一点点的这个准确性,	1202.98
但是只要把这个查询场,	1204.98
就是只要把这个总结点数量稍微调高一点,	1206.98
其实就可以了,	1208.98
另外的话呢,	1210.98
这个metagraph刚刚提到它有三种比对方式,	1212.98
那么这三种比对方式呢,	1216.98
首先是一个叫metagraph-aligned,	1218.98
metagraph-aligned其实就是序列与这个joint de Bruin graph去进行比对,	1221.98
然后去计算期在途中最接近的路径,	1226.98
那么它的特点其实就是允许比对到跨越不同注释标签的序列的一个叫重组路径,	1229.98
比对完成之后,	1235.98
结果路径就会用于获取相应的这个注释信息,	1236.98
那么第二个就是叫SCA,	1240.98
就是标签一致的图比对,	1243.98
那么这个比对方式,	1245.98
这个比对方式是针对不希望出现标签重组的情况,	1247.98
那么这时候呢,	1250.98
它查询序列和这个由单个注释标签导出的联合图的子图去进行比对,	1252.98
那么这种方法,	1258.98
这种方法特点就是这个通过一次搜索过程进行比对,	1260.98
同时它为跟踪与比对结果相对应的注释,	1264.98
然后确保比对结果和特定样本的属性是一致的,	1266.98
第三个就是这个TCG,	1270.98
就是轨迹一致的图比对,	1272.98
那么它是针对,	1274.98
就是原始输入序列索引当中的,	1276.98
就是已经对原始输入序列的索引进行的无损编码了,	1280.98
那么这个比对是针对了图中的轨迹去进行的,	1285.98
也是这个怎么说呢,	1288.98
它的特点就是比对针对图中的代表原始输入序列这个比对的轨迹去进行的,	1290.98
也就是它是专门适用于在无损编码情况下的东西去做的,	1299.98
那么接下来呢,	1308.23
这个还有一个叫P4查询算法,	1310.23
那么P4查询算法它的做法其实就是首先你会拿到一些查询序列,	1313.23
那么接下来你会把它分块分P,	1320.23
然后你会给每个P呢,	1322.23
每个P4去构建一个中间P4图,	1324.23
那基本上P4查询算法针对的是比如说这种测序数据,	1326.23
然后测序数据会有比如说很多条之间可能它的重合度比较大,	1330.23
因为它可能是来自于同一个这个环境里面的,	1337.23
那么这样的话呢,	1341.23
你如果去每条去像那个debrun graph里面去查的话,	1343.23
它会牺牲很多的计算资源,	1350.23
那么所以说它会把比如说相近的几个这个sequence把它去做一个中间P4图,	1353.23
就是先建立一个小的debrun graph,	1361.23
然后用这个小的debrun graph,	1364.23
和这个我们这个已经建立这个索引中的联合的这个joint的图像去进行有效的交集运算,	1366.23
这样的话就可以生成一个相对较小的子图,	1375.23
然后称其为查询图,	1378.23
那么这个查询图以这个hdbg,	1380.23
也就是维加索的格式保存,	1382.23
并且它包含了相应的注释信息,	1384.23
那么最后呢,	1386.23
就是对该P4中的所有的查询序列都针对这个比较比较小的这个查询图进行搜索,	1387.23
就懒得,	1393.23
就就就省得在那个比较大的图,	1394.23
在那个比较大的那个联合图上都进行搜索,	1395.23
那么这就是它P4查询算法的一个优化之点,	1398.23
优化点,	1402.23
那么接下来看看它的这个优势吧,	1403.23
那么它的优势首先就是高准确性,	1406.23
其次呢就是通用性,	1409.23
它通用性就是说你对DNA sequence,	1410.23
RNA sequence,	1412.23
还有一些蛋白质的那种东西都可以去进行查询,	1413.23
那么第三个就是它做,	1417.23
它说它做了模块化的设计,	1419.23
以至于这个如果其中每每个算法不好用,	1421.23
就把它把它拆开来同写一遍就是了,	1424.23
第四个就是这个它全部开源吧,	1427.23
就是它在SS3那个云存储上可以获取全部的索引,	1430.23
以及它有个网页服务去支持实时的交互式的查询,	1434.23
那么之后就是一些这个图表,	1438.23
就比如说就是展示一下它的这个优势,	1442.23
那么它的优势是这样的,	1446.23
就是比如说这张图里面就展示了这个number of reads in index,	1449.23
那么,	1453.23
这个是横轴,	1454.23
这个纵轴呢就是这个索引的大小,	1455.23
那么可以看到这个metagraph的索引大小在所有的工具比例当中都是属于最小的,	1457.23
那么另外呢就是它的这个这个加载索引和进行查询的这个时间,	1462.23
那么也是在所有在所有的这个这个这个reading size的大小上,	1468.23
以及在所有的这个它所选的这个对比工具之间它是最小的一个,	1474.23
那么另外的话就是可以看到它这个input,	1479.23
它就是随着input size,	1483.23
这个number of claimers in index的就是这个,	1485.23
这个东西的一个一个一个一个索引的大小的一个一个一个可视化,	1492.23
那么基本上可以看到它的这个,	1499.23
它的这个进行的索引之后的,	1503.23
它的这个索引大小是,	1507.23
就是就是叫什么,	1510.23
它是一个四线性的,	1512.23
就是这是一根参考线,	1514.23
然后它基本上是低于这个参考线上,	1516.23
然后可以看到它在比如说100TBP的这个这个状态下,	1519.23
它其实还是可以做到在这个100GB附近的这个怎么说呢,	1525.23
这个index size,	1532.23
然后接下来的其他的图呢就是展示一下它的这个这个召回率吧,	1534.23
这个这个这个这个召回率呢,	1545.24
就是它针对不,	1548.24
这个其实没有跟别的工具比对了,	1550.24
它它是自己跟自己,	1552.24
它它它它是在这个不同的这个这个这个物种之间是进行比对,	1554.24
那么这个比的话比较有意思,	1561.24
它比的意思就是它的这个随着这个变异率的提高,	1563.24
这个mutation rate的提高,	1566.24
那么可以看它这个exact match和aligned的这个平均召回率的这个表现差异,	1568.24
那么那个aligned的话,	1574.24
它是其实是容许一定的变异程度的,	1576.24
它其实是寻找那个最长的公共匹配路径,	1578.24
所以说在这个随着这个mutation rate从0升到0.02的时候,	1581.24
它这个aligned的这个下降其实不是很不是很高,	1585.24
那么基本上所有的这个这个average recall在这个任意的一个物种物种下面,	1590.24
在aligned的情况下其实都是高于75%的,	1600.24
但是这个这个这个数值是挺不错的,	1603.24
除了在这个netazor,	1606.24
这我也不知道啥,	1608.24
反正在这个变异率是0.02的时候,	1609.24
它会低于75%,	1612.24
然后另外有一些图就是表明它这个cost比较低,	1616.09
然后这边就展示一下它的cost,	1620.09
但是这个地方好像怎么说呢,	1623.09
没有和别的这个工具进行比较,	1626.09
所以也很难说吧,	1629.09
最后呢,	1632.09
最后这个c图呢,	1634.09
其实这块我没有太看懂,	1638.09
也没太搞明白它在说什么东西,	1641.09
就是我想一想,	1644.09
它这边x轴其实是展示的是最小前期匹配数,	1649.47
然后y轴是展示的就是随机访问号匹配的预期数量,	1652.47
那么我对它怎么说呢,	1657.47
大概理解就是它评估了这个随机的查询区别,	1659.47
比如说100BP到250BP,	1662.47
在整个公共序列集合当中的偶然匹配的预期数量,	1664.47
然后它这边就对比的这个精确匹配,	1668.47
比对和理论模型的预测结果,	1672.47
但是我不太清楚它说明了什么东西,	1675.47
可能是用于评估不同搜索策略的特异性吧,	1679.47
那么这个是它的一个cost和accuracy上的一个优势,	1682.47
那么当然它还是有一些这个问题的,	1688.47
就比如说我们刚刚说的就是,	1691.47
它有一步这个pruning操作,	1692.47
也就是它会对原始数据去进行一个修剪,	1694.47
那么一修剪它就很可能做不到,	1697.47
它论文后续提到它说它是无损压缩,	1700.47
但实际上不是,	1705.47
它一开始去进行的这个测序造成的去除,	1706.47
那好就好在这个去造了,	1708.47
坏就坏在,	1710.47
万一有一些就是其实并不是测试的问题,	1711.47
其实就是里面有一些比较新型的这种,	1716.47
然后比较少的这个cammer,	1719.47
这个cammer也被它去掉了,	1721.47
所以其实是有损的,	1723.47
第二就是它的数据结构是静态的,	1725.47
那么其实大规模更新的这个数据库的时候,	1727.47
比如说你想再加一点这个sequence进去的话,	1730.47
整个缩影需要完全重建,	1733.47
或者它给的这种方法就是,	1735.47
它把新增数据单独拉出来做一个缩影,	1737.47
然后查询的话分开查询,	1739.47
那这样的话显然就不是太好,	1741.47
然后第三个就是可能就是一个通病吧,	1743.47
就是远距离同源序列灵感性有限,	1745.47
这个就是这个这个这个,	1748.47
因为电,	1750.47
因为电信太大了,	1751.47
它也查不太到,	1752.47
所以可以理解吧,	1753.47
那么这个就是整个一个metagraph的一个,	1754.47
一个大致的讲,	1757.47
大致的一个讲,	1759.47
讲解,	1760.47
讲完了是吧,	1761.47
对的,	1773.09
好,	1775.09
那个是它有问题吗,	1776.09
就是它这个查询的时候,	1779.09
相当于要查的是序列完全一样吗,	1783.09
还是说它有一点小不一样,	1786.09
它有两种,	1788.09
它有两种查询模式,	1789.09
我反正,	1790.09
一个叫Kimmer精确查询,	1791.09
一个叫序列倒图比对,	1794.09
序列倒图比对其实就是这个,	1799.09
是识别图中最接近的匹配路径,	1802.09
就是容许一些变异的,	1806.09
行,	1809.09
然后像上次说的那个LexiMap,	1814.09
就是它就慢慢搜,	1816.09
然后会有一个这个,	1819.09
这个叫什么来着,	1820.09
就是匹配的score,	1822.09
然后它要做的事情就是用DP算法去最大化这个score,	1823.09
大概就是这个,	1828.09
好的,	1830.09
然后第二个问题是,	1831.09
就是在那个索引构建部分,	1832.09
应该是比较前面那个地方,	1834.09
OK,	1837.09
就是这里,	1839.09
我看看我,	1842.44
就是这中间的这一部分,	1844.44
就是它这些,	1849.23
好像有一些侧链是,	1851.23
被删掉了是吗,	1852.23
对的,	1855.23
这个,	1856.23
这个是,	1857.23
就是这里说的那个,	1858.23
给定,	1862.99
短于给定阈值的末端就给它修掉,	1863.99
是这个,	1865.99
对,	1866.99
有的是短于给定阈值的末端修掉,	1867.99
有的是这cammer的风度太低了,	1869.99
以及它不属于任何的高风度的组里面,	1872.99
然后它就会被丢掉,	1875.99
那这样子,	1876.99
也就是这一部分,	1880.62
我就之后觉得它可能有点隐患,	1884.92
就是它可能会把真实的,	1886.92
就是它的这个模型建立的假设是,	1888.92
你在这个进行这个什么,	1891.92
你在进行DNS,	1894.92
那个,	1896.92
那个,	1897.92
叫什么来着,	1898.92
就是,	1899.92
就是你在测试的时候,	1900.92
可能会有一些东西测得不对,	1901.92
那么它测得不对,	1903.92
认为就是这个东西风度比较低,	1904.92
所以它把风度低的都当做测得不对的,	1907.92
但是我觉得这是不太对的,	1909.92
另外它这个应该是适合做,	1913.92
各种各样的物种的序列吧,	1915.92
包括微生物,	1918.92
包括其他各种人啊,	1919.92
什么什么的,	1921.92
对对对,	1922.92
那它这个2K,	1923.92
可能对于某一些微生物来说,	1926.92
可能有点短了,	1928.92
我感觉,	1930.92
然后另外还有一个问题就是,	1934.92
它在这个图里面,	1936.92
它会存这些标签吗,	1937.92
就比如说哪一条链是来自于人的,	1939.92
哪一条链是来自哪个数据库这样的,	1942.92
哦,它是这样的,	1944.92
它是那个,	1946.92
它是那个叫什么,	1947.92
它所有组成,	1949.92
一个是那个Kimmer字典,	1950.92
就是这个Debrun graph,	1951.92
然后接下来还有下面有一个这个注释矩阵,	1953.92
这个注释矩阵才是真正的存那个什么信息的地方,	1956.92
就是上面只是存图信息,	1959.92
就是这个,	1961.92
就这个Kimmer跟Kimmer之间的关系,	1963.92
或者说它存不存在于这个,	1965.92
这个你的这个数据集里面,	1967.92
但是真正要看,	1969.92
比如说你给定一条secret,	1971.92
就是一个Kimmer对应的信息,	1973.92
比如说它是从什么地方来的,	1975.92
或者说它的日期什么东西的,	1976.92
它就全部是通过这个注释矩阵完成的,	1978.92
哦,好的,	1983.4
那这样子的话,	1985.4
那实际上相当于要存两大张表了,	1986.4
我感觉这个消耗内存可能还是有点,	1989.4
其实是三张,	1992.4
就是它第三张就没关系,	1993.4
它第三张就是,	1995.4
就比如说你这个注释矩阵已经存了,	1997.4
那么就是以第一个Kimmer,	1999.4
在,	2003.4
就具有第几个属性,	2004.4
你当然也应该会有一张表去存第几个属性具体是什么,	2006.4
嗯,	2009.4
哦,	2010.4
对,	2011.4
那确实,	2012.4
所以其实是三张表,	2013.4
确实,	2018.37
感觉这个查取好像也不是特别好做,	2019.37
数据量大了之后就会是这个样子,	2022.37
对对,	2024.37
它唯一就是,	2025.37
它唯一就是在那边掰扯的事情就是,	2026.37
它觉得这个注释矩阵它可以做的很稀熟,	2028.37
因为它用这个叫Row Difference的一个,	2031.37
这个一个技术,	2033.37
至于这个每两列之间它只存差异,	2035.37
不存这个,	2039.37
不存绝对的0和1,	2040.37
那么这样的话就可以把1的数量显著减少,	2042.37
那么接下来又因为它是个稀数矩阵,	2045.37
所以说它用稀数矩阵专门存法,	2047.37
这样的话整个数据量的存储就会比较小,	2049.37
我,	2055.0
我这边没有别的问题了,	2056.0
啊,	2059.95
我可能,	2060.95
对,	2061.95
我就觉得它做这么复杂,	2062.95
它到底,	2063.95
最大的好处到底是啥,	2064.95
就是这个东西,	2066.95
哦,	2067.95
就是一方面它就是说这个它可以存的很少,	2068.95
就是它对PB字节的数据可以压到很小,	2071.95
就是压缩比例还是比较低的,	2074.95
对,	2076.95
那它有没有什么省事呢,	2077.95
它的省事到底是什么呢,	2079.95
呃,	2082.95
不好意思,	2083.95
我,	2084.95
我,	2085.95
我,	2086.95
我,	2087.95
我,	2088.95
我,	2089.95
我,	2090.95
我,	2091.95
我,	2092.95
我,	2093.95
我,	2094.95
我,	2095.95
我,	2096.95
我,	2097.95
我,	2098.95
我,	2099.95
我,	2103.44
我,	2107.93
我,	2108.93
我,	2109.93
我,	2110.93
我,	2111.93
我,	2112.93
我,	2113.93
我,	2114.93
我,	2115.93
我,	2116.93
我,	2117.93
我,	2118.93
我,	2119.93
我,	2120.93
我,	2121.93
我,	2122.93
我,	2123.93
我,	2124.93
我,	2125.93
我,	2126.93
我,	2127.93
我,	2128.93
我,	2129.93
我,	2130.93
我,	2131.93
它的去除自主噪声的方式是有一个threshold	2132.93
刚刚在这里	2138.43
这里	2139.59
它的想法其实就是对于低风度的Kmer就直接删	2144.38
要么这个Kmer自己低风度	2150.86
要么就是它所在的Unitake里面风度比较低	2155.08
那么这样的话就会被删掉	2160.18
它觉得低风的是噪音	2161.68
但是也有可能是正确的	2164.88
对	2167.1
这一步就感觉比较随意	2167.64
你看后面它对比的时候对比了哪些方法	2173.38
世界对比	2186.36
或者你等一下你就看看刚才这个图	2188.71
讲讲这个图到底想说明啥	2193.55
比如说左边中间右边分别想说啥	2197.63
好	2200.37
好	2201.37
好	2202.65
好	2205.13
好	2205.35
就是它左边比如说先从一个人这个sample里面去得到了一串这个	2205.37
每个图要构一个构建一个W金图吗	2211.37
它是对每一个sample去先构建一个	2214.59
对每个样本就要构建一个区别构建一个W金图	2219.64
对吧	2223.52
构建好之后	2223.92
对	2224.92
构建好了之后	2225.34
它就在里面	2226.92
它就去应用这个叫叫prune	2227.9
就是这个图清洗	2230.7
那么图清洗就是首先它看看哪些风度比较低	2231.84
然后把风度低的删了	2235.12
对	2236.7
然后接下来呢	2237.2
它就是看一下这个什么	2238.34
这个其实我不太	2239.9
就是它把末端tip给修剪掉了	2241.0
把短于就是比如说这个短于2kbps的这个末端tip给修剪掉	2244.84
这就很神奇	2250.42
嗯	2251.98
然后接下来就会得到一个	2255.86
它为什么要修剪掉	2258.38
这修剪掉的是指了啥	2259.24
呃	2260.76
我没太看明白它修剪的逻辑是什么	2263.71
一定	2266.75
好	2275.46
那个	2276.52
它修剪之后呢	2277.34
然后再干嘛就是	2279.3
就是磨叽了一起	2281.9
是把这个图磨叽了一起	2282.94
对对对对	2284.3
其实其实其实它这个sample就是这边看起来比较简单	2287.6
但其实它没有没有看起来那么简单	2291.5
那么它之后会把这个样本图先分解成这个context或者unitx	2293.54
就是这副样子	2297.64
就是一根像线像就就串成一根串的样子	2299.14
然后之后把这一系列这个unitx把它再合并起来	2303.44
嗯	2308.3
那这个跟那个潘基伦graph的区别是啥	2308.84
你你知道潘基伦那个史达尼约他这跟潘基伦graph的区别是啥	2313.44
他这个应该就是	2322.27
这就是kmer里它是前一个直接接着后一个的就相当于他只每次只移动了	2325.77
一个剪辑吧就是德布路径图嘛那德布路径都是这样的我的意思潘基伦graph他也是个德布路径图构建的吧应该也是的	2332.97
我不知道这两个区别是啥	2341.27
我感觉他这里存的这个是比较原始的就没有把比如说连续的三个给他合并成一个长的这种	2342.97
不过本质上应该和潘基伦graph应该是差不多的	2363.31
就他这里额外在存了一些其他的注视信息之类的	2368.31
那这个	2378.59
他说的表示他就做好了之后就可以做检索那个拼接和对比对了	2379.79
那他跟传统的那些比对善法或者传统的拼接善法	2392.38
关系什么就是在这个技术上他就可以做还是什么就是这个东西	2399.48
他应该就可以直接做就是查评序列 然后做比对	2406.18
这个事情	2411.48
有个人这篇文章他主要想说的就是现在的数据库实在是太大了 然后直接马上断掉了 重新进来一下	2412.38
这样我那个合适 然后那个就是他把这个建了之后 我的感觉还蛮像潘基伦graph的 对吧	2468.44
那在这个技术上呢	2478.04
他又做了一个压缩的表示 然后他就可以做好多应用 你看后面他那个比较是	2480.24
嗯 kmo	2487.04
diction	2488.14
atation index	2488.44
好看后面有个比较的是吧 他好像跟有一几个方法比较过 对吧	2490.54
对 他刚才那个比较那个前面那也是不也有	2499.97
这个	2504.27
嗯	2506.07
对 他不是下面有比较吗 右边那几个那几个都是干嘛的 他跟他的区别	2509.69
好像还有很多人做这种东西 为什么我这些东西都没怎么带听说 他们都是	2516.19
在干嘛的	2521.79
amble for latest index	2524.19
older time	2526.29
他就说他的效率比较高	2530.5
exact match	2539.36
很烂	2545.43
好 ok吧 那个有什么问题在好 下面我再看看有什么图	2551.82
这个不是跟别人比对 这个不是跟别人比对 这个全是跟自己的	2562.18
嗯 我看看 我看看这个	2568.92
好 这这一页里面	2573.96
左边这个意思是想说啥 就是说那个微生物啊 真菌呢 什么就建的图是不一样是吧 还是什么意思	2577.26
还是说减数的时候	2587.26
是他是评估在不同的sra数据机索引上去进行实现发现查询的准确性	2590.56
然后这个然后他他他其实想比较的是那个align exact match在两种搜索类型 就是这两种搜索类型下的性能差异	2597.46
他其实没有跟别人做比对	2607.46
example会稍微弱一点点 然后这个那个align会好一点	2610.26
他他其实他其实整个三张图其实就是强表明这个我的align很好	2624.92
我的align相对于自己的exact match有有多少多少好处 但是他没有做evolution 就是没有做跟别的这个论文的这个对比	2630.22
下一页是啥 没了还是这这这这下也就是说他cost很低	2645.98
exact match	2655.13
嗯	2657.23
align	2660.83
为什么exact match还这么低align还高一些	2662.53
我真怀他说写我真怀他说写错了	2666.73
应该也没有 他可能有什么别的原因 ok	2670.43
就是有点奇怪是吧	2675.03
哦 我知道他可能exact match他可能那个东西可能很好做 他可能就是在那个矩阵里面怎么样比对就完全一样	2678.03
那其他的可能	2686.53
你就算起来更麻烦	2687.23
ok吧 那个下面还是什么	2698.26
那边别就这个这个就是他这个最后一张图的 为什么这个东西对于我们平时有啥用吗 就这个东西他做好了之后有啥用呢 大家有什么用啊对他对他来说	2701.16
呃 是说对我们的工作还是说这个就是这个项目本身这个项目到底意义在哪里	2714.06
他的意义应该就是聚合了一大堆这个数数数据库 然后说他可以把很多非常大的数据给压缩	2721.26
然后去做应用也还快	2727.86
对 嗯 然后他提供了一个在线交入式的平台 以及那个把可可以把收银下载的网址 但是他那个在线平台好像已经爆掉了 就是现在nl	2732.16
好 ok好 呃 世道有什么问题没有	2745.86
哦 我基本上都问过	2749.86
好 ok 那就先先暂时这样 好的 有什么问题再说 暗示呗 ok	2751.96
啊 是他你什么要说的吗	2757.86
那那老师等你待会我单独跟你说一下	2761.06
什么老师 待会我单独跟你说一下好的好的 ok好 要不我先退出 然后好的好的 那现在你先退吧嗯 ok ok 再见嗯	2765.56