WEBVTT

00:00:20.719 --> 00:00:27.539
好的,那个谁讲,可以听得听吗?

00:00:30.179 --> 00:00:31.660
可以的,可以的。

00:01:10.819 --> 00:01:12.299
那个是谁讲吗,诗涛?

00:01:14.299 --> 00:01:17.519
是我讲,我讲那个论文,Metagraph。

00:01:19.700 --> 00:01:21.060
我共享一下屏幕。

00:01:31.489 --> 00:01:37.310
OK,我来讲一下这篇这个Metagraph,

00:01:37.310 --> 00:01:40.310
然后Metagraph它其实就是,

00:01:40.769 --> 00:01:43.810
我觉得跟上次学的那个Lexi,

00:01:43.810 --> 00:01:47.409
就上次我看的那篇LexiMap还是有一点相像之处的,

00:01:47.409 --> 00:01:51.409
但是我感觉比LexiMap好像更加的robust一点。

00:01:51.409 --> 00:01:58.409
那么它是一个在大规模生物序列库中实现高效并且准确的这个全文本搜索的一个框架。

00:01:59.010 --> 00:02:03.109
然后以下是这篇文章的作者,

00:02:03.109 --> 00:02:08.009
然后基本上都是从ETH这个学校的。

00:02:10.409 --> 00:02:13.710
然后他所遇到的这个主要的挑战首先是,

00:02:13.710 --> 00:02:15.909
这个数据库的规模报道性的增长。

00:02:15.909 --> 00:02:19.909
然后第二个就是它有一个全文本搜索的不可行性,

00:02:19.909 --> 00:02:22.009
也就是长期以来这个,

00:02:22.009 --> 00:02:30.409
就是Petabase,就是Pet字节级别的这个原始测试数据是一直没有办法进行全文本搜索的。

00:02:30.409 --> 00:02:37.710
那它这里的全文本搜索的意思是检索所有包含与给定的查询序列相似序列的数据集。

00:02:37.710 --> 00:02:41.310
那么这篇文章就为了解决,

00:02:41.310 --> 00:02:43.310
就需要解决这个框架。

00:02:43.710 --> 00:02:44.710
就需要解决这个问题。

00:02:44.710 --> 00:02:46.810
那么还有一些现有的工具。

00:02:48.610 --> 00:02:52.310
现有的工具有比如说这个草图技术。

00:02:52.310 --> 00:02:53.810
那么这个草图技术呢,

00:02:53.810 --> 00:02:59.610
就是基于这个哈希算法去生成这个数据的一个比较紧凑的摘要。

00:02:59.610 --> 00:03:05.610
然后这个摘要去用来去做近似级的相似性和包含的查询。

00:03:05.610 --> 00:03:10.409
但是他提的一个例子就是叫Pebble Scout这个工具。

00:03:10.409 --> 00:03:13.210
然后虽然说他觉得这个扩展性很强,

00:03:13.210 --> 00:03:17.310
但是他会缺乏比较敏感的比对支持,

00:03:17.310 --> 00:03:19.810
并且存在比较高的假阳性率。

00:03:19.810 --> 00:03:22.710
这个是草图技术,草图技术局限。

00:03:22.710 --> 00:03:27.409
另外一个叫这个Approximate Membership Query Data Structure。

00:03:27.409 --> 00:03:32.610
然后他的代表的一个技术是Bloom Filters。

00:03:32.610 --> 00:03:38.310
那么他主要是用于对序列集合进行单个短序列的近似查询。

00:03:38.310 --> 00:03:43.009
但是这边提到的比如说COBS方法和KM index,

00:03:43.009 --> 00:03:47.810
他也存在有比较强的这种假阳性的匹配问题。

00:03:47.810 --> 00:03:52.610
也就是他们会错误的报告一个序列是存在于数据集当中的。

00:03:52.610 --> 00:03:56.210
然后这样的话就不太好。

00:03:56.210 --> 00:04:00.610
第三个就是精确表示的带注式的Debruin图。

00:04:00.610 --> 00:04:03.509
那么这种做法是可以保证精确性,

00:04:03.509 --> 00:04:07.710
但是它在可扩展性和查询功能多样性之间难以保持平衡。

00:04:07.710 --> 00:04:12.210
那么现有的这个带注式Debruin图的查询功能多样性比较少。

00:04:12.210 --> 00:04:12.810
也就是,

00:04:12.810 --> 00:04:17.009
很可能只能支持比如说KM的精确匹配。

00:04:17.009 --> 00:04:24.410
所以说这篇论文就是基于这个精确表示的带注式Debruin图的这个想法,

00:04:24.410 --> 00:04:31.379
去进一步深挖得到的一个比较完整的框架。

00:04:31.379 --> 00:04:33.779
那么Metagraph其实它的核心想法,

00:04:33.779 --> 00:04:37.980
也就是说它其实主要做的事情就是去构建了一个索引。

00:04:37.980 --> 00:04:40.579
那么这个索引它是两块东西组成的。

00:04:40.579 --> 00:04:43.980
那么第一块就是一个叫KM字典的东西。

00:04:43.980 --> 00:04:45.079
那么这个KM字典,

00:04:45.079 --> 00:04:47.879
其实本质上就是一个Debruin图。

00:04:47.879 --> 00:04:49.879
可以看看右边,

00:04:49.879 --> 00:04:50.779
这个这块,

00:04:50.779 --> 00:04:51.879
右上角这块,

00:04:51.879 --> 00:04:54.579
其实就是他们构建的一个Debruin Graph。

00:04:54.579 --> 00:04:56.680
这个Debruin Graph具体是什么东西呢?

00:04:56.680 --> 00:04:58.779
就是它会存,

00:04:58.779 --> 00:05:02.180
它会根据KM的信息,

00:05:02.180 --> 00:05:04.079
把这个不同的,

00:05:04.079 --> 00:05:12.879
把这个存在K典一个重合的这个KM用一根有象的线段连起来。

00:05:12.879 --> 00:05:14.079
那么这样的话,

00:05:14.079 --> 00:05:14.879
整个一个,

00:05:14.879 --> 00:05:20.279
整个一个数据库就会被以一种图的形式存下来。

00:05:20.279 --> 00:05:22.079
然后存的过程中,

00:05:22.079 --> 00:05:24.680
其实这个有象的线段是不需要存,

00:05:24.680 --> 00:05:25.680
它其实只要存,

00:05:25.680 --> 00:05:28.279
就比如说这个这个TCG,

00:05:28.279 --> 00:05:30.279
CCT和GTA就可以了,

00:05:30.279 --> 00:05:34.279
它并不需要存中间这个连接的这根线段,

00:05:34.279 --> 00:05:40.480
因为通过存储的这个set就可以还原出来整个Graph的一个样子。

00:05:40.480 --> 00:05:42.579
然后这是KM字典,

00:05:42.579 --> 00:05:44.680
那么它既然还需要,

00:05:44.680 --> 00:05:47.180
就是一些别的信息,

00:05:47.180 --> 00:05:51.079
就比如说你给它一个query查询的一个段,

00:05:51.079 --> 00:05:52.480
然后你需要知道它,

00:05:52.480 --> 00:05:53.680
比如说这个,

00:05:53.680 --> 00:05:56.579
这个这一段基因是从哪里来的,

00:05:56.579 --> 00:05:58.779
以及这段基因可能拥有的其他信息,

00:05:58.779 --> 00:06:00.879
包括但不限于这个分类信息什么的,

00:06:00.879 --> 00:06:03.480
所以它还需要一个注视,

00:06:03.480 --> 00:06:05.279
它还需要一层注视,

00:06:05.279 --> 00:06:07.079
那么为了这个需求,

00:06:07.079 --> 00:06:10.079
它就是构建了一个叫注视矩阵的东西,

00:06:10.079 --> 00:06:13.379
和这个上面提到这个Debruing Graphs联合在一起,

00:06:13.379 --> 00:06:14.379
就可以,

00:06:14.680 --> 00:06:20.180
就理论上可以完成这个序列的查询工作。

00:06:20.180 --> 00:06:24.079
那么接下来可以继续想一想的Kimmer Set,

00:06:24.079 --> 00:06:28.579
这个Kimmer Set它这个存储的方式,

00:06:28.579 --> 00:06:30.180
它这边提到了三种,

00:06:30.180 --> 00:06:33.180
那么首先是一个叫Hash DBG的东西,

00:06:33.180 --> 00:06:34.879
这个Hash DBG呢,

00:06:34.879 --> 00:06:36.879
就是主要用于内部的操作,

00:06:36.879 --> 00:06:39.379
比如说P处理的序列查找,

00:06:39.379 --> 00:06:41.779
然后第二个叫Bitmap DBG,

00:06:41.779 --> 00:06:43.180
就是它把Kimmer,

00:06:43.180 --> 00:06:47.579
因为Kimmer的这个可能的Kimmer其实数量是有限的,

00:06:47.579 --> 00:06:50.279
那么它就把它存储成一个二进制向量,

00:06:50.279 --> 00:06:54.879
就表示哪一些Kimmer存在于这个集合当中。

00:06:54.879 --> 00:06:58.579
那么第三个东西就是Succinct DBG,

00:06:58.579 --> 00:07:01.779
那么它是基于一个东西叫BOSS表构写的,

00:07:01.779 --> 00:07:06.480
这BOSS其实是四个人的首字母加在一起,

00:07:06.480 --> 00:07:10.980
然后这其实是类似于一个就是点七,

00:07:10.980 --> 00:07:12.279
就是类似点七的,

00:07:12.279 --> 00:07:14.680
或者点ZIP的一种压缩文件格式,

00:07:14.680 --> 00:07:17.480
但是是专门为了这种DNA sequence,

00:07:17.480 --> 00:07:19.980
RNA sequence去做的,

00:07:19.980 --> 00:07:22.079
那么使用这种构件呢,

00:07:22.079 --> 00:07:23.980
会表现出最佳的压缩性能,

00:07:23.980 --> 00:07:26.779
那么这也是作为默认的一种压缩表示的,

00:07:26.779 --> 00:07:32.420
用来减少这个索引的大小,

00:07:32.420 --> 00:07:34.720
然后接下来就说一说这个,

00:07:34.720 --> 00:07:39.019
去具体说一说这个注释矩阵它是怎么做的,

00:07:39.019 --> 00:07:43.120
那么它的注释矩阵其实就是一个N乘M的一个,

00:07:43.120 --> 00:07:44.420
N乘M的一个01,

00:07:44.620 --> 00:07:48.519
那么N乘M的01比如说长这样,

00:07:48.519 --> 00:07:54.060
然后我们看比如说它DI行,

00:07:54.060 --> 00:07:56.360
DJ列,

00:07:56.360 --> 00:08:00.509
如果DI行DJ列这个东西是1呢,

00:08:00.509 --> 00:08:08.209
那说明了就是这个从列来看是这个K门,

00:08:08.209 --> 00:08:10.310
从行来看呢就是这个数,

00:08:10.310 --> 00:08:12.110
就是D这个属性,

00:08:12.110 --> 00:08:16.810
那么它的意思就是DI个K门会具有D这个属性,

00:08:16.810 --> 00:08:17.709
它是用D,

00:08:17.709 --> 00:08:19.610
用这样的方式来,

00:08:19.610 --> 00:08:21.810
然后它当然旁边可能还会有张表,

00:08:21.810 --> 00:08:25.509
就比如说DJ一个属性,

00:08:25.509 --> 00:08:29.819
那么具体的属性是什么,

00:08:29.819 --> 00:08:34.120
那么它通过这一张I乘J的这张表01表,

00:08:34.120 --> 00:08:40.419
其实就表明了比如说具体的哪个K门,

00:08:40.419 --> 00:08:43.220
哪些K门会具有J的这个属性,

00:08:43.220 --> 00:08:47.720
那么从这个图上其实大概可以看出来它里面会存在很多0,

00:08:47.720 --> 00:08:49.919
所以说这个矩阵其实是,

00:08:49.919 --> 00:08:50.620
规模比较庞大,

00:08:50.620 --> 00:08:51.820
但通常极其稀疏的,

00:08:51.820 --> 00:08:55.320
那么论文中也提到它的这个列数一般有10的12次方,

00:08:55.320 --> 00:08:58.419
然后它的行数大概有10的9次方附近,

00:08:58.419 --> 00:08:59.620
所以它确实很庞大,

00:08:59.620 --> 00:09:01.419
但是正是因为它是个稀疏矩阵,

00:09:01.419 --> 00:09:03.419
所以可以说比较高效的压缩,

00:09:03.419 --> 00:09:10.120
这个压缩基本上就是选取就是专门的用来存稀疏矩阵这个存法,

00:09:10.120 --> 00:09:16.679
这个也比较直观吧,

00:09:16.679 --> 00:09:22.490
然后接下来就,

00:09:22.490 --> 00:09:24.490
之前其实我们讲了这个,

00:09:24.590 --> 00:09:30.590
这个K文这个set以及入式矩阵的这个定义吧,

00:09:30.590 --> 00:09:35.590
那么接下来我们就看一下它这个具体是怎么去构建这两个东西,

00:09:35.590 --> 00:09:37.590
那么要构建这两个东西呢,

00:09:37.590 --> 00:09:42.590
它首先做的第一步是去做这个叫数据预处理,

00:09:42.590 --> 00:09:43.590
这个数据预处理呢,

00:09:43.590 --> 00:09:48.789
其实就是从原始输入样本中去构建单图图的这个debrun图,

00:09:48.789 --> 00:09:50.789
那么也就是如右图中所示的,

00:09:50.789 --> 00:09:53.490
比如说从这个男的女的,

00:09:53.490 --> 00:09:54.389
还有动物,

00:09:54.389 --> 00:09:55.190
植物,

00:09:55.190 --> 00:09:58.889
这个不同的样本上面去构建单独的debrun图,

00:09:58.889 --> 00:10:00.389
就是这张小图,

00:10:00.389 --> 00:10:04.389
那么它可选的一步步骤是对,

00:10:04.389 --> 00:10:09.389
它可选但推进的一步步骤是对每一个样本图去进行一次叫图清洗的,

00:10:09.389 --> 00:10:13.289
就是图片上边写的这个optional graph cleaning,

00:10:13.289 --> 00:10:16.889
叫它用的是pop bubbles和prune tips的方法,

00:10:16.889 --> 00:10:23.389
那么那说人话的就是它基于这个cammer分度阈值去识别和修剪这个虚假的路径,

00:10:23.389 --> 00:10:29.389
也就是比如说如果哪一个cammer它的这个风度比较低的话,

00:10:29.389 --> 00:10:30.389
它就会把它,

00:10:30.389 --> 00:10:33.389
它就会把prune tips把它删掉,

00:10:33.389 --> 00:10:35.389
那么清洗它是去,

00:10:35.389 --> 00:10:40.389
这个清洗决定基于cammer所属的unitake中的这个中位度分数,

00:10:40.389 --> 00:10:46.389
那么低风度的这个cammer如果位于只有足够多高风度cammer的unitake中,

00:10:46.389 --> 00:10:47.389
它还是会被保证,

00:10:47.389 --> 00:10:49.389
否则它们会被丢弃,

00:10:49.389 --> 00:10:52.389
然后第三个就是它会把短余给定阈值,

00:10:52.389 --> 00:10:57.389
它通常写的是2000个这个检测队的这个末端tips给修剪掉,

00:10:57.389 --> 00:11:04.389
那么清洗后的这个样本图就会被分解成一系列的这个comptics或者unitics,

00:11:04.389 --> 00:11:08.389
那么这些序列会用于后续去构建这个联合图,

00:11:08.389 --> 00:11:12.389
也就是中间这块东西,

00:11:12.389 --> 00:11:15.850
好,

00:11:15.850 --> 00:11:18.850
那么数据集我们先预处理过了,

00:11:18.850 --> 00:11:22.850
然后接下来就开始构建这个大图了,

00:11:22.850 --> 00:11:24.850
那么其实这一步也没什么好说,

00:11:24.850 --> 00:11:28.850
就是把所有的在第一阶段获得的这个comptics或者unitics去合并,

00:11:28.850 --> 00:11:32.850
去形成一个单一的这个叫joint de Bruin graph,

00:11:32.850 --> 00:11:36.850
那么它在这一步操作里面其实会做一些优化,

00:11:36.850 --> 00:11:40.850
比如说当它所引来自未知链的原始读取的时候,

00:11:40.850 --> 00:11:43.850
这个metagraph会构建一个叫canonical graph,

00:11:43.850 --> 00:11:49.850
然后这个graph当中的每一个cammer以及它的反向互补序列都会被包含在里面,

00:11:49.850 --> 00:11:51.850
那么为了提高压缩效率,

00:11:51.850 --> 00:11:54.850
它的构建主图的这个机制里面是这样的,

00:11:54.850 --> 00:12:00.850
它只储存每对cammer及其反向互补序列中的一个方向,

00:12:00.850 --> 00:12:04.850
然后这个影视表示的另一个方向其实就是不表示,

00:12:04.850 --> 00:12:09.850
那么这样的话它就可以在简洁的表示,

00:12:09.850 --> 00:12:11.850
比如说就是基于之前所说的boss表,

00:12:11.850 --> 00:12:14.850
这种简洁表示中去实现高效的压缩,

00:12:14.850 --> 00:12:16.850
去避免添加额外的虚拟cammer,

00:12:16.850 --> 00:12:25.350
然后构建完了de Bruin的那个graph,

00:12:25.350 --> 00:12:28.350
之后我们就要去构建那个之前所说的灵异矩阵,

00:12:28.350 --> 00:12:29.350
但是这里插一嘴,

00:12:29.350 --> 00:12:31.350
其实它这个不一定是灵异矩阵,

00:12:31.350 --> 00:12:35.350
它那个也可以是有值的矩阵去包含更多的信息,

00:12:35.350 --> 00:12:38.350
这是在论文之后的地方所说的,

00:12:38.350 --> 00:12:42.350
那么在这个de Bruin graph构建完成之后,

00:12:42.350 --> 00:12:44.350
它可以去做并行的叠,

00:12:44.350 --> 00:12:46.350
它可以并行的叠在不同的样本,

00:12:46.350 --> 00:12:53.350
并且把每个样本的所有cammer映射到联合图上去生成单个的注视列,

00:12:53.350 --> 00:12:57.350
那么注视矩阵随后会被压缩成最适合目标应用的表示形式,

00:12:57.350 --> 00:13:01.350
也就是之前所说的要去表示一个系数矩阵,

00:13:01.350 --> 00:13:06.350
那么首先是它采用一个叫row difference的压缩技术,

00:13:06.350 --> 00:13:09.350
那么这个压缩技术其实主要考虑的一点是,

00:13:09.350 --> 00:13:12.350
这个de Bruin graph当中相邻的节点,

00:13:12.350 --> 00:13:14.350
它的注视其实也应该是相似的,

00:13:14.350 --> 00:13:16.350
这也是比较符合知觉的一点,

00:13:16.350 --> 00:13:19.350
就是你毕竟这个剪辑差不太多,

00:13:19.350 --> 00:13:23.350
然后它所对应的注视其实也差不太多,

00:13:23.350 --> 00:13:26.350
所以说它接下来作为一部信息的压缩工作,

00:13:26.350 --> 00:13:34.350
其实是把节点的原始注视计划成与它后继的节点对应的注视之间的相似程度,

00:13:34.350 --> 00:13:41.350
那么通过把原始的信息改成相对差异,

00:13:41.350 --> 00:13:46.350
那么它可以节省很多注视和注视之间重叠的部分,

00:13:46.350 --> 00:13:51.350
这样的话整个注视矩阵可能从本来的1比较多,

00:13:51.350 --> 00:13:52.350
变成1比较少,

00:13:52.350 --> 00:13:55.350
它只存了一个比如说差异量,

00:13:55.350 --> 00:14:00.350
那么这样的话可以把它的压缩性能进步的提高,

00:14:00.350 --> 00:14:03.350
那么接下来就是它的矩阵表示方案了,

00:14:03.350 --> 00:14:05.350
那么它这边就提了比如说Row Spars,

00:14:05.350 --> 00:14:13.350
Column Compressed和Multi-BRWT这三种注视矩阵表示方式,

00:14:13.350 --> 00:14:16.350
那么它的推荐其实就是在稀疏化之后,

00:14:16.350 --> 00:14:21.350
列转换为Multi-BRWT或者是Row Spars表示,

00:14:21.350 --> 00:14:24.350
然后这样的话可以平衡速度和内存需求,

00:14:24.350 --> 00:14:28.350
另外一种Column Compressed它对内存需求比较高,

00:14:28.350 --> 00:14:33.850
那么之前我们讲了这个整个索引的构建过程,

00:14:33.850 --> 00:14:35.850
那么接下来讲讲它的查询,

00:14:35.850 --> 00:14:38.850
不好意思我喝过水,

00:14:38.850 --> 00:14:45.809
OK,

00:14:45.809 --> 00:14:47.809
在这个Metagraph它的查询操作,

00:14:47.809 --> 00:14:49.809
就是之前说,

00:14:49.809 --> 00:14:53.809
就是之前有说别的工具可能只支持这个精确CAMERA匹配,

00:14:53.809 --> 00:14:55.809
那么它做的好的地方就是在于,

00:14:55.809 --> 00:14:56.809
首先是它精确CAMERA匹配,

00:14:56.809 --> 00:14:57.809
一定是可以的,

00:14:57.809 --> 00:15:01.809
那么它里面牵涉了一些优化的技术,

00:15:01.809 --> 00:15:06.809
那么第二个就是它可以做序列到图匹配,

00:15:06.809 --> 00:15:09.809
那么这个东西其实是针对变异性比较高的数据,

00:15:09.809 --> 00:15:13.809
也就是它并不是一个完全精确的匹配,

00:15:13.809 --> 00:15:18.809
它就是匹配这个与这个给定的这个索引库里面,

00:15:18.809 --> 00:15:21.809
匹配最长的这个路径,

00:15:21.809 --> 00:15:23.809
然后它用的方式就是取一个Seed,

00:15:23.809 --> 00:15:24.809
然后Extend,

00:15:24.809 --> 00:15:28.809
这有点像上次提到的那个LexiMap,

00:15:28.809 --> 00:15:33.809
第三个就是它可以去做这个Batched Sequence Search,

00:15:33.809 --> 00:15:37.809
就是做P量化的这个查询算法,

00:15:37.809 --> 00:15:41.809
然后它也对P量化的查询算法做了一些优化,

00:15:41.809 --> 00:15:45.809
接下来就是先讲一下这个精确CAMERA匹配,

00:15:45.809 --> 00:15:49.809
那么精确CAMERA匹配里面它所用到的东西,

00:15:49.809 --> 00:15:52.809
其实就是当映射CAMERA到主图的时候,

00:15:52.809 --> 00:15:53.809
如果查询的CAMERA没有找到,

00:15:53.809 --> 00:15:58.809
那么就很可能是它的这个反向互补CAMERA存在的,

00:15:58.809 --> 00:16:01.809
那么它会去优化这个查询顺序,

00:16:01.809 --> 00:16:07.809
并且对之后的查询使用这个反向互补序列去进行查询,

00:16:07.809 --> 00:16:12.809
那么第二个就是它这个图如果是在Boss表当中去进行表示的时候,

00:16:12.809 --> 00:16:14.809
它去做一步操作教学,

00:16:14.809 --> 00:16:17.809
它会去索引这个CAMERA在Boss表中的范围,

00:16:17.809 --> 00:16:23.220
这样的话可以加快CAMERA的这个查找速度,

00:16:23.220 --> 00:16:25.220
然后接下来就是它,

00:16:25.220 --> 00:16:27.220
这个重头戏,

00:16:27.220 --> 00:16:29.220
就是序列到图的匹配,

00:16:29.220 --> 00:16:33.220
那么这个序列到图的匹配它识别图中最接近的匹配路径的比对方法,

00:16:33.220 --> 00:16:35.220
是用Seed and Extend的方法,

00:16:35.220 --> 00:16:38.220
那么它其实就是先找一个Seed出来,

00:16:38.220 --> 00:16:44.220
然后这个Seed的种子就是Unity里面连续的CAMERA匹配组成的,

00:16:44.220 --> 00:16:45.220
那么第二个就是Extend,

00:16:45.220 --> 00:16:49.220
那么它每个种子就会在图内向前和向后扩展,

00:16:49.220 --> 00:16:52.220
用来生成就是比较完整的一个局部的比对,

00:16:52.220 --> 00:16:54.220
然后它其实这边主要用的想法就是,

00:16:54.220 --> 00:16:56.220
这个DP跟上次的Extend比较像,

00:16:56.220 --> 00:16:59.220
然后比对方式的话它会提供三种比对方式,

00:16:59.220 --> 00:17:03.220
那么我们接下来来比较详细的讲一下这个Seed and Extend的方法,

00:17:03.220 --> 00:17:06.220
那么找种子的话,

00:17:06.220 --> 00:17:08.220
如果长的种子就没什么好处,

00:17:08.220 --> 00:17:09.220
就找着设,

00:17:09.220 --> 00:17:10.220
但是它说,

00:17:10.220 --> 00:17:11.220
它这个比较特殊的是,

00:17:11.220 --> 00:17:13.220
它如果寻查,

00:17:13.220 --> 00:17:16.220
它如果查找这个短于K的种子,

00:17:16.220 --> 00:17:18.220
它会执行这个三步,

00:17:18.220 --> 00:17:21.220
那么第一步是它会去搜索查询这个,

00:17:21.220 --> 00:17:23.220
就是当这个K比较小于K的时候,

00:17:23.220 --> 00:17:25.220
K一般取的是这个长度是19,

00:17:25.220 --> 00:17:27.220
那么在小于19的情况下,

00:17:27.220 --> 00:17:30.220
它搜索去查询这个是正向KMERS,

00:17:30.220 --> 00:17:33.220
然后之后它会去反向互补去找一下,

00:17:33.220 --> 00:17:37.220
最后它通过图的便利去确保找到这个后缀的匹配,

00:17:37.220 --> 00:17:43.220
那么这个就是它针对这个短的种子去进行的一些优化,

00:17:43.220 --> 00:17:45.220
接下来是扩展阶段,

00:17:45.220 --> 00:17:46.220
这个比较重要,

00:17:46.220 --> 00:17:50.220
就是它的扩展阶段首先是定义了三个分属向量,

00:17:50.220 --> 00:17:52.220
一个是S一个F,

00:17:52.220 --> 00:17:58.220
然后S这三个向量的长度都等于查询的长度L,

00:17:58.220 --> 00:18:01.220
那么比如说你针对一个具体的I而言,

00:18:01.220 --> 00:18:05.220
SI就是存储的查询前缀S1到SI,

00:18:05.220 --> 00:18:09.220
也就是结束到节点B的最佳比对分数,

00:18:09.220 --> 00:18:17.980
然后这个EI和FI就分别代表以插入和删除,

00:18:17.980 --> 00:18:20.980
在结束于节点B的最佳比对分数,

00:18:20.980 --> 00:18:22.980
这个其实就已经,

00:18:22.980 --> 00:18:28.980
这个建模其实已经很像这个GP的这种格式了,

00:18:28.980 --> 00:18:32.980
那么接下来它其实做的事情就是去,

00:18:32.980 --> 00:18:35.980
它其实就是从种子的起始界点,

00:18:35.980 --> 00:18:36.980
比如VS开始,

00:18:36.980 --> 00:18:38.980
它会去构建一颗比对数,

00:18:38.980 --> 00:18:45.980
那么这个比对数会编码所有的这个从VS开始的这个所有路径,

00:18:45.980 --> 00:18:51.980
然后另外这个比对数TS它这个size可能会很大,

00:18:51.980 --> 00:18:54.980
一种最坏的情况就是这个图是循环的,

00:18:54.980 --> 00:18:56.980
那么这个TS可能是无限大,

00:18:56.980 --> 00:18:59.980
它这个数可以不停的往下延伸,

00:18:59.980 --> 00:19:02.980
那么接下来它就说,

00:19:02.980 --> 00:19:05.980
我既然这个大小无限大,

00:19:05.980 --> 00:19:07.980
我计算资源不能无限大,

00:19:07.980 --> 00:19:09.980
所以它要引入一些这个heuristic,

00:19:09.980 --> 00:19:11.980
一些启发式的限制,

00:19:11.980 --> 00:19:15.980
启发式的限制它提了一下三个准则,

00:19:15.980 --> 00:19:20.980
那么首先是比如说一个元素的分数低于当前计算的就是最佳比对分数,

00:19:20.980 --> 00:19:23.980
然后如果低了超过X了,

00:19:23.980 --> 00:19:25.980
那么这个元素就会被跳过,

00:19:25.980 --> 00:19:31.980
第二个准则就是对于图节点的这个聚合分数,

00:19:31.980 --> 00:19:36.980
它叫aggregate scores,

00:19:36.980 --> 00:19:39.980
那么如果你遍离这个节点,

00:19:39.980 --> 00:19:41.980
但是这个聚合分数没有被更新,

00:19:41.980 --> 00:19:46.980
那么它也会被从这个VS那个数里面去丢弃掉,

00:19:46.980 --> 00:19:48.980
那么第三个就是节点探索限制,

00:19:48.980 --> 00:19:49.980
也就是对于这个数,

00:19:49.980 --> 00:19:52.980
它这个总深度会有一个怎么说呢,

00:19:52.980 --> 00:19:54.980
它会有个限制,

00:19:54.980 --> 00:19:58.980
所以说这三个准则加在一起就确保了,

00:19:58.980 --> 00:20:02.980
它可以在这个比较短的时间里面去完成整个数的匹配,

00:20:02.980 --> 00:20:04.980
当然它丢失一点点的这个准确性,

00:20:04.980 --> 00:20:06.980
但是只要把这个查询场,

00:20:06.980 --> 00:20:08.980
就是只要把这个总结点数量稍微调高一点,

00:20:08.980 --> 00:20:10.980
其实就可以了,

00:20:10.980 --> 00:20:12.980
另外的话呢,

00:20:12.980 --> 00:20:16.980
这个metagraph刚刚提到它有三种比对方式,

00:20:16.980 --> 00:20:18.980
那么这三种比对方式呢,

00:20:18.980 --> 00:20:21.980
首先是一个叫metagraph-aligned,

00:20:21.980 --> 00:20:26.980
metagraph-aligned其实就是序列与这个joint de Bruin graph去进行比对,

00:20:26.980 --> 00:20:29.980
然后去计算期在途中最接近的路径,

00:20:29.980 --> 00:20:35.980
那么它的特点其实就是允许比对到跨越不同注释标签的序列的一个叫重组路径,

00:20:35.980 --> 00:20:36.980
比对完成之后,

00:20:36.980 --> 00:20:40.980
结果路径就会用于获取相应的这个注释信息,

00:20:40.980 --> 00:20:43.980
那么第二个就是叫SCA,

00:20:43.980 --> 00:20:45.980
就是标签一致的图比对,

00:20:45.980 --> 00:20:47.980
那么这个比对方式,

00:20:47.980 --> 00:20:50.980
这个比对方式是针对不希望出现标签重组的情况,

00:20:50.980 --> 00:20:52.980
那么这时候呢,

00:20:52.980 --> 00:20:58.980
它查询序列和这个由单个注释标签导出的联合图的子图去进行比对,

00:20:58.980 --> 00:21:00.980
那么这种方法,

00:21:00.980 --> 00:21:04.980
这种方法特点就是这个通过一次搜索过程进行比对,

00:21:04.980 --> 00:21:06.980
同时它为跟踪与比对结果相对应的注释,

00:21:06.980 --> 00:21:10.980
然后确保比对结果和特定样本的属性是一致的,

00:21:10.980 --> 00:21:12.980
第三个就是这个TCG,

00:21:12.980 --> 00:21:14.980
就是轨迹一致的图比对,

00:21:14.980 --> 00:21:16.980
那么它是针对,

00:21:16.980 --> 00:21:20.980
就是原始输入序列索引当中的,

00:21:20.980 --> 00:21:25.980
就是已经对原始输入序列的索引进行的无损编码了,

00:21:25.980 --> 00:21:28.980
那么这个比对是针对了图中的轨迹去进行的,

00:21:28.980 --> 00:21:30.980
也是这个怎么说呢,

00:21:30.980 --> 00:21:39.980
它的特点就是比对针对图中的代表原始输入序列这个比对的轨迹去进行的,

00:21:39.980 --> 00:21:47.230
也就是它是专门适用于在无损编码情况下的东西去做的,

00:21:48.230 --> 00:21:50.230
那么接下来呢,

00:21:50.230 --> 00:21:53.230
这个还有一个叫P4查询算法,

00:21:53.230 --> 00:22:00.230
那么P4查询算法它的做法其实就是首先你会拿到一些查询序列,

00:22:00.230 --> 00:22:02.230
那么接下来你会把它分块分P,

00:22:02.230 --> 00:22:04.230
然后你会给每个P呢,

00:22:04.230 --> 00:22:06.230
每个P4去构建一个中间P4图,

00:22:06.230 --> 00:22:10.230
那基本上P4查询算法针对的是比如说这种测序数据,

00:22:10.230 --> 00:22:17.230
然后测序数据会有比如说很多条之间可能它的重合度比较大,

00:22:17.230 --> 00:22:21.230
因为它可能是来自于同一个这个环境里面的,

00:22:21.230 --> 00:22:23.230
那么这样的话呢,

00:22:23.230 --> 00:22:30.230
你如果去每条去像那个debrun graph里面去查的话,

00:22:30.230 --> 00:22:33.230
它会牺牲很多的计算资源,

00:22:33.230 --> 00:22:41.230
那么所以说它会把比如说相近的几个这个sequence把它去做一个中间P4图,

00:22:41.230 --> 00:22:44.230
就是先建立一个小的debrun graph,

00:22:44.230 --> 00:22:46.230
然后用这个小的debrun graph,

00:22:46.230 --> 00:22:55.230
和这个我们这个已经建立这个索引中的联合的这个joint的图像去进行有效的交集运算,

00:22:55.230 --> 00:22:58.230
这样的话就可以生成一个相对较小的子图,

00:22:58.230 --> 00:23:00.230
然后称其为查询图,

00:23:00.230 --> 00:23:02.230
那么这个查询图以这个hdbg,

00:23:02.230 --> 00:23:04.230
也就是维加索的格式保存,

00:23:04.230 --> 00:23:06.230
并且它包含了相应的注释信息,

00:23:06.230 --> 00:23:07.230
那么最后呢,

00:23:07.230 --> 00:23:13.230
就是对该P4中的所有的查询序列都针对这个比较比较小的这个查询图进行搜索,

00:23:13.230 --> 00:23:14.230
就懒得,

00:23:14.230 --> 00:23:15.230
就就就省得在那个比较大的图,

00:23:15.230 --> 00:23:18.230
在那个比较大的那个联合图上都进行搜索,

00:23:18.230 --> 00:23:22.230
那么这就是它P4查询算法的一个优化之点,

00:23:22.230 --> 00:23:23.230
优化点,

00:23:23.230 --> 00:23:26.230
那么接下来看看它的这个优势吧,

00:23:26.230 --> 00:23:29.230
那么它的优势首先就是高准确性,

00:23:29.230 --> 00:23:30.230
其次呢就是通用性,

00:23:30.230 --> 00:23:32.230
它通用性就是说你对DNA sequence,

00:23:32.230 --> 00:23:33.230
RNA sequence,

00:23:33.230 --> 00:23:37.230
还有一些蛋白质的那种东西都可以去进行查询,

00:23:37.230 --> 00:23:39.230
那么第三个就是它做,

00:23:39.230 --> 00:23:41.230
它说它做了模块化的设计,

00:23:41.230 --> 00:23:44.230
以至于这个如果其中每每个算法不好用,

00:23:44.230 --> 00:23:47.230
就把它把它拆开来同写一遍就是了,

00:23:47.230 --> 00:23:50.230
第四个就是这个它全部开源吧,

00:23:50.230 --> 00:23:54.230
就是它在SS3那个云存储上可以获取全部的索引,

00:23:54.230 --> 00:23:58.230
以及它有个网页服务去支持实时的交互式的查询,

00:23:58.230 --> 00:24:02.230
那么之后就是一些这个图表,

00:24:02.230 --> 00:24:06.230
就比如说就是展示一下它的这个优势,

00:24:06.230 --> 00:24:09.230
那么它的优势是这样的,

00:24:09.230 --> 00:24:13.230
就是比如说这张图里面就展示了这个number of reads in index,

00:24:13.230 --> 00:24:14.230
那么,

00:24:14.230 --> 00:24:15.230
这个是横轴,

00:24:15.230 --> 00:24:17.230
这个纵轴呢就是这个索引的大小,

00:24:17.230 --> 00:24:22.230
那么可以看到这个metagraph的索引大小在所有的工具比例当中都是属于最小的,

00:24:22.230 --> 00:24:28.230
那么另外呢就是它的这个这个加载索引和进行查询的这个时间,

00:24:28.230 --> 00:24:34.230
那么也是在所有在所有的这个这个这个reading size的大小上,

00:24:34.230 --> 00:24:39.230
以及在所有的这个它所选的这个对比工具之间它是最小的一个,

00:24:39.230 --> 00:24:43.230
那么另外的话就是可以看到它这个input,

00:24:43.230 --> 00:24:45.230
它就是随着input size,

00:24:45.230 --> 00:24:52.230
这个number of claimers in index的就是这个,

00:24:52.230 --> 00:24:59.230
这个东西的一个一个一个一个索引的大小的一个一个一个可视化,

00:24:59.230 --> 00:25:03.230
那么基本上可以看到它的这个,

00:25:03.230 --> 00:25:07.230
它的这个进行的索引之后的,

00:25:07.230 --> 00:25:10.230
它的这个索引大小是,

00:25:10.230 --> 00:25:12.230
就是就是叫什么,

00:25:12.230 --> 00:25:14.230
它是一个四线性的,

00:25:14.230 --> 00:25:16.230
就是这是一根参考线,

00:25:16.230 --> 00:25:19.230
然后它基本上是低于这个参考线上,

00:25:19.230 --> 00:25:25.230
然后可以看到它在比如说100TBP的这个这个状态下,

00:25:25.230 --> 00:25:32.230
它其实还是可以做到在这个100GB附近的这个怎么说呢,

00:25:32.230 --> 00:25:34.230
这个index size,

00:25:34.230 --> 00:25:44.240
然后接下来的其他的图呢就是展示一下它的这个这个召回率吧,

00:25:45.240 --> 00:25:48.240
这个这个这个这个召回率呢,

00:25:48.240 --> 00:25:50.240
就是它针对不,

00:25:50.240 --> 00:25:52.240
这个其实没有跟别的工具比对了,

00:25:52.240 --> 00:25:54.240
它它是自己跟自己,

00:25:54.240 --> 00:26:01.240
它它它它是在这个不同的这个这个这个物种之间是进行比对,

00:26:01.240 --> 00:26:03.240
那么这个比的话比较有意思,

00:26:03.240 --> 00:26:06.240
它比的意思就是它的这个随着这个变异率的提高,

00:26:06.240 --> 00:26:08.240
这个mutation rate的提高,

00:26:08.240 --> 00:26:14.240
那么可以看它这个exact match和aligned的这个平均召回率的这个表现差异,

00:26:14.240 --> 00:26:16.240
那么那个aligned的话,

00:26:16.240 --> 00:26:18.240
它是其实是容许一定的变异程度的,

00:26:18.240 --> 00:26:21.240
它其实是寻找那个最长的公共匹配路径,

00:26:21.240 --> 00:26:25.240
所以说在这个随着这个mutation rate从0升到0.02的时候,

00:26:25.240 --> 00:26:30.240
它这个aligned的这个下降其实不是很不是很高,

00:26:30.240 --> 00:26:40.240
那么基本上所有的这个这个average recall在这个任意的一个物种物种下面,

00:26:40.240 --> 00:26:43.240
在aligned的情况下其实都是高于75%的,

00:26:43.240 --> 00:26:46.240
但是这个这个这个数值是挺不错的,

00:26:46.240 --> 00:26:48.240
除了在这个netazor,

00:26:48.240 --> 00:26:49.240
这我也不知道啥,

00:26:49.240 --> 00:26:52.240
反正在这个变异率是0.02的时候,

00:26:52.240 --> 00:26:56.089
它会低于75%,

00:26:56.089 --> 00:27:00.089
然后另外有一些图就是表明它这个cost比较低,

00:27:00.089 --> 00:27:03.089
然后这边就展示一下它的cost,

00:27:03.089 --> 00:27:06.089
但是这个地方好像怎么说呢,

00:27:06.089 --> 00:27:09.089
没有和别的这个工具进行比较,

00:27:09.089 --> 00:27:12.089
所以也很难说吧,

00:27:12.089 --> 00:27:14.089
最后呢,

00:27:14.089 --> 00:27:18.089
最后这个c图呢,

00:27:18.089 --> 00:27:21.089
其实这块我没有太看懂,

00:27:21.089 --> 00:27:24.089
也没太搞明白它在说什么东西,

00:27:24.089 --> 00:27:29.470
就是我想一想,

00:27:29.470 --> 00:27:32.470
它这边x轴其实是展示的是最小前期匹配数,

00:27:32.470 --> 00:27:37.470
然后y轴是展示的就是随机访问号匹配的预期数量,

00:27:37.470 --> 00:27:39.470
那么我对它怎么说呢,

00:27:39.470 --> 00:27:42.470
大概理解就是它评估了这个随机的查询区别,

00:27:42.470 --> 00:27:44.470
比如说100BP到250BP,

00:27:44.470 --> 00:27:48.470
在整个公共序列集合当中的偶然匹配的预期数量,

00:27:48.470 --> 00:27:52.470
然后它这边就对比的这个精确匹配,

00:27:52.470 --> 00:27:55.470
比对和理论模型的预测结果,

00:27:55.470 --> 00:27:59.470
但是我不太清楚它说明了什么东西,

00:27:59.470 --> 00:28:02.470
可能是用于评估不同搜索策略的特异性吧,

00:28:02.470 --> 00:28:08.470
那么这个是它的一个cost和accuracy上的一个优势,

00:28:08.470 --> 00:28:11.470
那么当然它还是有一些这个问题的,

00:28:11.470 --> 00:28:12.470
就比如说我们刚刚说的就是,

00:28:12.470 --> 00:28:14.470
它有一步这个pruning操作,

00:28:14.470 --> 00:28:17.470
也就是它会对原始数据去进行一个修剪,

00:28:17.470 --> 00:28:20.470
那么一修剪它就很可能做不到,

00:28:20.470 --> 00:28:25.470
它论文后续提到它说它是无损压缩,

00:28:25.470 --> 00:28:26.470
但实际上不是,

00:28:26.470 --> 00:28:28.470
它一开始去进行的这个测序造成的去除,

00:28:28.470 --> 00:28:30.470
那好就好在这个去造了,

00:28:30.470 --> 00:28:31.470
坏就坏在,

00:28:31.470 --> 00:28:36.470
万一有一些就是其实并不是测试的问题,

00:28:36.470 --> 00:28:39.470
其实就是里面有一些比较新型的这种,

00:28:39.470 --> 00:28:41.470
然后比较少的这个cammer,

00:28:41.470 --> 00:28:43.470
这个cammer也被它去掉了,

00:28:43.470 --> 00:28:45.470
所以其实是有损的,

00:28:45.470 --> 00:28:47.470
第二就是它的数据结构是静态的,

00:28:47.470 --> 00:28:50.470
那么其实大规模更新的这个数据库的时候,

00:28:50.470 --> 00:28:53.470
比如说你想再加一点这个sequence进去的话,

00:28:53.470 --> 00:28:55.470
整个缩影需要完全重建,

00:28:55.470 --> 00:28:57.470
或者它给的这种方法就是,

00:28:57.470 --> 00:28:59.470
它把新增数据单独拉出来做一个缩影,

00:28:59.470 --> 00:29:01.470
然后查询的话分开查询,

00:29:01.470 --> 00:29:03.470
那这样的话显然就不是太好,

00:29:03.470 --> 00:29:05.470
然后第三个就是可能就是一个通病吧,

00:29:05.470 --> 00:29:08.470
就是远距离同源序列灵感性有限,

00:29:08.470 --> 00:29:10.470
这个就是这个这个这个,

00:29:10.470 --> 00:29:11.470
因为电,

00:29:11.470 --> 00:29:12.470
因为电信太大了,

00:29:12.470 --> 00:29:13.470
它也查不太到,

00:29:13.470 --> 00:29:14.470
所以可以理解吧,

00:29:14.470 --> 00:29:17.470
那么这个就是整个一个metagraph的一个,

00:29:17.470 --> 00:29:19.470
一个大致的讲,

00:29:19.470 --> 00:29:20.470
大致的一个讲,

00:29:20.470 --> 00:29:21.470
讲解,

00:29:21.470 --> 00:29:33.089
讲完了是吧,

00:29:33.089 --> 00:29:35.089
对的,

00:29:35.089 --> 00:29:36.089
好,

00:29:36.089 --> 00:29:39.089
那个是它有问题吗,

00:29:39.089 --> 00:29:43.089
就是它这个查询的时候,

00:29:43.089 --> 00:29:46.089
相当于要查的是序列完全一样吗,

00:29:46.089 --> 00:29:48.089
还是说它有一点小不一样,

00:29:48.089 --> 00:29:49.089
它有两种,

00:29:49.089 --> 00:29:50.089
它有两种查询模式,

00:29:50.089 --> 00:29:51.089
我反正,

00:29:51.089 --> 00:29:54.089
一个叫Kimmer精确查询,

00:29:54.089 --> 00:29:59.089
一个叫序列倒图比对,

00:29:59.089 --> 00:30:02.089
序列倒图比对其实就是这个,

00:30:02.089 --> 00:30:06.089
是识别图中最接近的匹配路径,

00:30:06.089 --> 00:30:09.089
就是容许一些变异的,

00:30:09.089 --> 00:30:14.089
行,

00:30:14.089 --> 00:30:16.089
然后像上次说的那个LexiMap,

00:30:16.089 --> 00:30:19.089
就是它就慢慢搜,

00:30:19.089 --> 00:30:20.089
然后会有一个这个,

00:30:20.089 --> 00:30:22.089
这个叫什么来着,

00:30:22.089 --> 00:30:23.089
就是匹配的score,

00:30:23.089 --> 00:30:28.089
然后它要做的事情就是用DP算法去最大化这个score,

00:30:28.089 --> 00:30:30.089
大概就是这个,

00:30:30.089 --> 00:30:31.089
好的,

00:30:31.089 --> 00:30:32.089
然后第二个问题是,

00:30:32.089 --> 00:30:34.089
就是在那个索引构建部分,

00:30:34.089 --> 00:30:37.089
应该是比较前面那个地方,

00:30:37.089 --> 00:30:39.089
OK,

00:30:39.089 --> 00:30:42.440
就是这里,

00:30:42.440 --> 00:30:44.440
我看看我,

00:30:44.440 --> 00:30:49.230
就是这中间的这一部分,

00:30:49.230 --> 00:30:51.230
就是它这些,

00:30:51.230 --> 00:30:52.230
好像有一些侧链是,

00:30:52.230 --> 00:30:55.230
被删掉了是吗,

00:30:55.230 --> 00:30:56.230
对的,

00:30:56.230 --> 00:30:57.230
这个,

00:30:57.230 --> 00:30:58.230
这个是,

00:30:58.230 --> 00:31:02.990
就是这里说的那个,

00:31:02.990 --> 00:31:03.990
给定,

00:31:03.990 --> 00:31:05.990
短于给定阈值的末端就给它修掉,

00:31:05.990 --> 00:31:06.990
是这个,

00:31:06.990 --> 00:31:07.990
对,

00:31:07.990 --> 00:31:09.990
有的是短于给定阈值的末端修掉,

00:31:09.990 --> 00:31:12.990
有的是这cammer的风度太低了,

00:31:12.990 --> 00:31:15.990
以及它不属于任何的高风度的组里面,

00:31:15.990 --> 00:31:16.990
然后它就会被丢掉,

00:31:16.990 --> 00:31:20.619
那这样子,

00:31:20.619 --> 00:31:24.920
也就是这一部分,

00:31:24.920 --> 00:31:26.920
我就之后觉得它可能有点隐患,

00:31:26.920 --> 00:31:28.920
就是它可能会把真实的,

00:31:28.920 --> 00:31:31.920
就是它的这个模型建立的假设是,

00:31:31.920 --> 00:31:34.920
你在这个进行这个什么,

00:31:34.920 --> 00:31:36.920
你在进行DNS,

00:31:36.920 --> 00:31:37.920
那个,

00:31:37.920 --> 00:31:38.920
那个,

00:31:38.920 --> 00:31:39.920
叫什么来着,

00:31:39.920 --> 00:31:40.920
就是,

00:31:40.920 --> 00:31:41.920
就是你在测试的时候,

00:31:41.920 --> 00:31:43.920
可能会有一些东西测得不对,

00:31:43.920 --> 00:31:44.920
那么它测得不对,

00:31:44.920 --> 00:31:47.920
认为就是这个东西风度比较低,

00:31:47.920 --> 00:31:49.920
所以它把风度低的都当做测得不对的,

00:31:49.920 --> 00:31:51.920
但是我觉得这是不太对的,

00:31:53.920 --> 00:31:55.920
另外它这个应该是适合做,

00:31:55.920 --> 00:31:58.920
各种各样的物种的序列吧,

00:31:58.920 --> 00:31:59.920
包括微生物,

00:31:59.920 --> 00:32:01.920
包括其他各种人啊,

00:32:01.920 --> 00:32:02.920
什么什么的,

00:32:02.920 --> 00:32:03.920
对对对,

00:32:03.920 --> 00:32:05.920
那它这个2K,

00:32:06.920 --> 00:32:08.920
可能对于某一些微生物来说,

00:32:08.920 --> 00:32:10.920
可能有点短了,

00:32:10.920 --> 00:32:11.920
我感觉,

00:32:14.920 --> 00:32:16.920
然后另外还有一个问题就是,

00:32:16.920 --> 00:32:17.920
它在这个图里面,

00:32:17.920 --> 00:32:19.920
它会存这些标签吗,

00:32:19.920 --> 00:32:22.920
就比如说哪一条链是来自于人的,

00:32:22.920 --> 00:32:24.920
哪一条链是来自哪个数据库这样的,

00:32:24.920 --> 00:32:26.920
哦,它是这样的,

00:32:26.920 --> 00:32:27.920
它是那个,

00:32:27.920 --> 00:32:29.920
它是那个叫什么,

00:32:29.920 --> 00:32:30.920
它所有组成,

00:32:30.920 --> 00:32:31.920
一个是那个Kimmer字典,

00:32:31.920 --> 00:32:33.920
就是这个Debrun graph,

00:32:33.920 --> 00:32:36.920
然后接下来还有下面有一个这个注释矩阵,

00:32:36.920 --> 00:32:39.920
这个注释矩阵才是真正的存那个什么信息的地方,

00:32:39.920 --> 00:32:41.920
就是上面只是存图信息,

00:32:41.920 --> 00:32:43.920
就是这个,

00:32:43.920 --> 00:32:45.920
就这个Kimmer跟Kimmer之间的关系,

00:32:45.920 --> 00:32:47.920
或者说它存不存在于这个,

00:32:47.920 --> 00:32:49.920
这个你的这个数据集里面,

00:32:49.920 --> 00:32:51.920
但是真正要看,

00:32:51.920 --> 00:32:53.920
比如说你给定一条secret,

00:32:53.920 --> 00:32:55.920
就是一个Kimmer对应的信息,

00:32:55.920 --> 00:32:56.920
比如说它是从什么地方来的,

00:32:56.920 --> 00:32:58.920
或者说它的日期什么东西的,

00:32:58.920 --> 00:33:03.400
它就全部是通过这个注释矩阵完成的,

00:33:03.400 --> 00:33:05.400
哦,好的,

00:33:05.400 --> 00:33:06.400
那这样子的话,

00:33:06.400 --> 00:33:09.400
那实际上相当于要存两大张表了,

00:33:09.400 --> 00:33:12.400
我感觉这个消耗内存可能还是有点,

00:33:12.400 --> 00:33:13.400
其实是三张,

00:33:13.400 --> 00:33:15.400
就是它第三张就没关系,

00:33:15.400 --> 00:33:17.400
它第三张就是,

00:33:17.400 --> 00:33:19.400
就比如说你这个注释矩阵已经存了,

00:33:19.400 --> 00:33:23.400
那么就是以第一个Kimmer,

00:33:23.400 --> 00:33:24.400
在,

00:33:24.400 --> 00:33:26.400
就具有第几个属性,

00:33:26.400 --> 00:33:29.400
你当然也应该会有一张表去存第几个属性具体是什么,

00:33:29.400 --> 00:33:30.400
嗯,

00:33:30.400 --> 00:33:31.400
哦,

00:33:31.400 --> 00:33:32.400
对,

00:33:32.400 --> 00:33:33.400
那确实,

00:33:33.400 --> 00:33:38.369
所以其实是三张表,

00:33:38.369 --> 00:33:39.369
确实,

00:33:39.369 --> 00:33:42.369
感觉这个查取好像也不是特别好做,

00:33:42.369 --> 00:33:44.369
数据量大了之后就会是这个样子,

00:33:44.369 --> 00:33:45.369
对对,

00:33:45.369 --> 00:33:46.369
它唯一就是,

00:33:46.369 --> 00:33:48.369
它唯一就是在那边掰扯的事情就是,

00:33:48.369 --> 00:33:51.369
它觉得这个注释矩阵它可以做的很稀熟,

00:33:51.369 --> 00:33:53.369
因为它用这个叫Row Difference的一个,

00:33:53.369 --> 00:33:55.369
这个一个技术,

00:33:55.369 --> 00:33:59.369
至于这个每两列之间它只存差异,

00:33:59.369 --> 00:34:00.369
不存这个,

00:34:00.369 --> 00:34:02.369
不存绝对的0和1,

00:34:02.369 --> 00:34:05.369
那么这样的话就可以把1的数量显著减少,

00:34:05.369 --> 00:34:07.369
那么接下来又因为它是个稀数矩阵,

00:34:07.369 --> 00:34:09.369
所以说它用稀数矩阵专门存法,

00:34:09.369 --> 00:34:15.000
这样的话整个数据量的存储就会比较小,

00:34:15.000 --> 00:34:16.000
我,

00:34:16.000 --> 00:34:19.949
我这边没有别的问题了,

00:34:19.949 --> 00:34:20.949
啊,

00:34:20.949 --> 00:34:21.949
我可能,

00:34:21.949 --> 00:34:22.949
对,

00:34:22.949 --> 00:34:23.949
我就觉得它做这么复杂,

00:34:23.949 --> 00:34:24.949
它到底,

00:34:24.949 --> 00:34:26.949
最大的好处到底是啥,

00:34:26.949 --> 00:34:27.949
就是这个东西,

00:34:27.949 --> 00:34:28.949
哦,

00:34:28.949 --> 00:34:31.949
就是一方面它就是说这个它可以存的很少,

00:34:31.949 --> 00:34:34.949
就是它对PB字节的数据可以压到很小,

00:34:34.949 --> 00:34:36.949
就是压缩比例还是比较低的,

00:34:36.949 --> 00:34:37.949
对,

00:34:37.949 --> 00:34:39.949
那它有没有什么省事呢,

00:34:39.949 --> 00:34:42.949
它的省事到底是什么呢,

00:34:42.949 --> 00:34:43.949
呃,

00:34:43.949 --> 00:34:44.949
不好意思,

00:34:44.949 --> 00:34:45.949
我,

00:34:45.949 --> 00:34:46.949
我,

00:34:46.949 --> 00:34:47.949
我,

00:34:47.949 --> 00:34:48.949
我,

00:34:48.949 --> 00:34:49.949
我,

00:34:49.949 --> 00:34:50.949
我,

00:34:50.949 --> 00:34:51.949
我,

00:34:51.949 --> 00:34:52.949
我,

00:34:52.949 --> 00:34:53.949
我,

00:34:53.949 --> 00:34:54.949
我,

00:34:54.949 --> 00:34:55.949
我,

00:34:55.949 --> 00:34:56.949
我,

00:34:56.949 --> 00:34:57.949
我,

00:34:57.949 --> 00:34:58.949
我,

00:34:58.949 --> 00:34:59.949
我,

00:34:59.949 --> 00:35:03.440
我,

00:35:03.440 --> 00:35:07.929
我,

00:35:07.929 --> 00:35:08.929
我,

00:35:08.929 --> 00:35:09.929
我,

00:35:09.929 --> 00:35:10.929
我,

00:35:10.929 --> 00:35:11.929
我,

00:35:11.929 --> 00:35:12.929
我,

00:35:12.929 --> 00:35:13.929
我,

00:35:13.929 --> 00:35:14.929
我,

00:35:14.929 --> 00:35:15.929
我,

00:35:15.929 --> 00:35:16.929
我,

00:35:16.929 --> 00:35:17.929
我,

00:35:17.929 --> 00:35:18.929
我,

00:35:18.929 --> 00:35:19.929
我,

00:35:19.929 --> 00:35:20.929
我,

00:35:20.929 --> 00:35:21.929
我,

00:35:21.929 --> 00:35:22.929
我,

00:35:22.929 --> 00:35:23.929
我,

00:35:23.929 --> 00:35:24.929
我,

00:35:24.929 --> 00:35:25.929
我,

00:35:25.929 --> 00:35:26.929
我,

00:35:26.929 --> 00:35:27.929
我,

00:35:27.929 --> 00:35:28.929
我,

00:35:28.929 --> 00:35:29.929
我,

00:35:29.929 --> 00:35:30.929
我,

00:35:30.929 --> 00:35:31.929
我,

00:35:31.929 --> 00:35:32.929
我,

00:35:32.929 --> 00:35:38.429
它的去除自主噪声的方式是有一个threshold

00:35:38.429 --> 00:35:39.590
刚刚在这里

00:35:39.590 --> 00:35:44.380
这里

00:35:44.380 --> 00:35:50.860
它的想法其实就是对于低风度的Kmer就直接删

00:35:50.860 --> 00:35:55.079
要么这个Kmer自己低风度

00:35:55.079 --> 00:36:00.179
要么就是它所在的Unitake里面风度比较低

00:36:00.179 --> 00:36:01.679
那么这样的话就会被删掉

00:36:01.679 --> 00:36:04.880
它觉得低风的是噪音

00:36:04.880 --> 00:36:07.099
但是也有可能是正确的

00:36:07.099 --> 00:36:07.639
对

00:36:07.639 --> 00:36:13.380
这一步就感觉比较随意

00:36:13.380 --> 00:36:26.360
你看后面它对比的时候对比了哪些方法

00:36:26.360 --> 00:36:28.710
世界对比

00:36:28.710 --> 00:36:33.550
或者你等一下你就看看刚才这个图

00:36:33.550 --> 00:36:37.630
讲讲这个图到底想说明啥

00:36:37.630 --> 00:36:40.369
比如说左边中间右边分别想说啥

00:36:40.369 --> 00:36:41.369
好

00:36:41.369 --> 00:36:42.650
好

00:36:42.650 --> 00:36:45.130
好

00:36:45.130 --> 00:36:45.349
好

00:36:45.349 --> 00:36:45.369
好

00:36:45.369 --> 00:36:51.369
就是它左边比如说先从一个人这个sample里面去得到了一串这个

00:36:51.369 --> 00:36:54.590
每个图要构一个构建一个W金图吗

00:36:54.590 --> 00:36:59.639
它是对每一个sample去先构建一个

00:36:59.639 --> 00:37:03.519
对每个样本就要构建一个区别构建一个W金图

00:37:03.519 --> 00:37:03.920
对吧

00:37:03.920 --> 00:37:04.920
构建好之后

00:37:04.920 --> 00:37:05.340
对

00:37:05.340 --> 00:37:06.920
构建好了之后

00:37:06.920 --> 00:37:07.900
它就在里面

00:37:07.900 --> 00:37:10.699
它就去应用这个叫叫prune

00:37:10.699 --> 00:37:11.840
就是这个图清洗

00:37:11.840 --> 00:37:15.119
那么图清洗就是首先它看看哪些风度比较低

00:37:15.119 --> 00:37:16.699
然后把风度低的删了

00:37:16.699 --> 00:37:17.179
对

00:37:17.199 --> 00:37:18.340
然后接下来呢

00:37:18.340 --> 00:37:19.900
它就是看一下这个什么

00:37:19.900 --> 00:37:21.000
这个其实我不太

00:37:21.000 --> 00:37:24.840
就是它把末端tip给修剪掉了

00:37:24.840 --> 00:37:30.420
把短于就是比如说这个短于2kbps的这个末端tip给修剪掉

00:37:30.420 --> 00:37:31.980
这就很神奇

00:37:31.980 --> 00:37:35.860
嗯

00:37:35.860 --> 00:37:38.380
然后接下来就会得到一个

00:37:38.380 --> 00:37:39.239
它为什么要修剪掉

00:37:39.239 --> 00:37:40.760
这修剪掉的是指了啥

00:37:40.760 --> 00:37:43.710
呃

00:37:43.710 --> 00:37:46.750
我没太看明白它修剪的逻辑是什么

00:37:46.750 --> 00:37:55.460
一定

00:37:55.460 --> 00:37:56.519
好

00:37:56.519 --> 00:37:57.340
那个

00:37:57.340 --> 00:37:59.300
它修剪之后呢

00:37:59.300 --> 00:38:01.300
然后再干嘛就是

00:38:01.900 --> 00:38:02.940
就是磨叽了一起

00:38:02.940 --> 00:38:04.300
是把这个图磨叽了一起

00:38:04.300 --> 00:38:06.300
对对对对

00:38:07.599 --> 00:38:11.500
其实其实其实它这个sample就是这边看起来比较简单

00:38:11.500 --> 00:38:13.539
但其实它没有没有看起来那么简单

00:38:13.539 --> 00:38:17.639
那么它之后会把这个样本图先分解成这个context或者unitx

00:38:17.639 --> 00:38:19.139
就是这副样子

00:38:19.139 --> 00:38:23.440
就是一根像线像就就串成一根串的样子

00:38:23.440 --> 00:38:27.440
然后之后把这一系列这个unitx把它再合并起来

00:38:28.300 --> 00:38:28.840
嗯

00:38:28.840 --> 00:38:32.039
那这个跟那个潘基伦graph的区别是啥

00:38:33.440 --> 00:38:37.340
你你知道潘基伦那个史达尼约他这跟潘基伦graph的区别是啥

00:38:42.269 --> 00:38:44.070
他这个应该就是

00:38:45.769 --> 00:38:52.050
这就是kmer里它是前一个直接接着后一个的就相当于他只每次只移动了

00:38:52.969 --> 00:39:00.610
一个剪辑吧就是德布路径图嘛那德布路径都是这样的我的意思潘基伦graph他也是个德布路径图构建的吧应该也是的

00:39:01.269 --> 00:39:02.769
我不知道这两个区别是啥

00:39:02.969 --> 00:39:20.170
我感觉他这里存的这个是比较原始的就没有把比如说连续的三个给他合并成一个长的这种

00:39:23.309 --> 00:39:26.610
不过本质上应该和潘基伦graph应该是差不多的

00:39:28.309 --> 00:39:32.510
就他这里额外在存了一些其他的注视信息之类的

00:39:38.590 --> 00:39:39.190
那这个

00:39:39.789 --> 00:39:49.980
他说的表示他就做好了之后就可以做检索那个拼接和对比对了

00:39:52.380 --> 00:39:57.980
那他跟传统的那些比对善法或者传统的拼接善法

00:39:59.480 --> 00:40:03.780
关系什么就是在这个技术上他就可以做还是什么就是这个东西

00:40:06.179 --> 00:40:10.880
他应该就可以直接做就是查评序列 然后做比对

00:40:11.480 --> 00:40:12.079
这个事情

00:40:12.380 --> 00:40:19.280
有个人这篇文章他主要想说的就是现在的数据库实在是太大了 然后直接马上断掉了 重新进来一下

00:41:08.440 --> 00:41:16.039
这样我那个合适 然后那个就是他把这个建了之后 我的感觉还蛮像潘基伦graph的 对吧

00:41:18.039 --> 00:41:19.440
那在这个技术上呢

00:41:20.239 --> 00:41:26.039
他又做了一个压缩的表示 然后他就可以做好多应用 你看后面他那个比较是

00:41:27.039 --> 00:41:27.840
嗯 kmo

00:41:28.139 --> 00:41:28.440
diction

00:41:28.440 --> 00:41:29.840
atation index

00:41:30.539 --> 00:41:34.840
好看后面有个比较的是吧 他好像跟有一几个方法比较过 对吧

00:41:39.969 --> 00:41:43.070
对 他刚才那个比较那个前面那也是不也有

00:41:44.269 --> 00:41:44.769
这个

00:41:46.070 --> 00:41:46.570
嗯

00:41:49.690 --> 00:41:54.789
对 他不是下面有比较吗 右边那几个那几个都是干嘛的 他跟他的区别

00:41:56.190 --> 00:42:01.789
好像还有很多人做这种东西 为什么我这些东西都没怎么带听说 他们都是

00:42:01.789 --> 00:42:03.190
在干嘛的

00:42:04.190 --> 00:42:05.489
amble for latest index

00:42:06.289 --> 00:42:07.489
older time

00:42:10.500 --> 00:42:11.800
他就说他的效率比较高

00:42:19.360 --> 00:42:20.260
exact match

00:42:25.429 --> 00:42:25.829
很烂

00:42:31.820 --> 00:42:37.019
好 ok吧 那个有什么问题在好 下面我再看看有什么图

00:42:42.179 --> 00:42:45.880
这个不是跟别人比对 这个不是跟别人比对 这个全是跟自己的

00:42:48.920 --> 00:42:53.260
嗯 我看看 我看看这个

00:42:53.960 --> 00:42:56.860
好 这这一页里面

00:42:57.260 --> 00:43:06.360
左边这个意思是想说啥 就是说那个微生物啊 真菌呢 什么就建的图是不一样是吧 还是什么意思

00:43:07.260 --> 00:43:08.559
还是说减数的时候

00:43:10.559 --> 00:43:15.659
是他是评估在不同的sra数据机索引上去进行实现发现查询的准确性

00:43:17.460 --> 00:43:26.860
然后这个然后他他他其实想比较的是那个align exact match在两种搜索类型 就是这两种搜索类型下的性能差异

00:43:27.460 --> 00:43:28.659
他其实没有跟别人做比对

00:43:30.260 --> 00:43:34.860
example会稍微弱一点点 然后这个那个align会好一点

00:43:44.920 --> 00:43:49.719
他他其实他其实整个三张图其实就是强表明这个我的align很好

00:43:50.219 --> 00:43:57.920
我的align相对于自己的exact match有有多少多少好处 但是他没有做evolution 就是没有做跟别的这个论文的这个对比

00:44:05.980 --> 00:44:10.579
下一页是啥 没了还是这这这这下也就是说他cost很低

00:44:15.130 --> 00:44:16.329
exact match

00:44:17.230 --> 00:44:19.230
嗯

00:44:20.829 --> 00:44:21.230
align

00:44:22.530 --> 00:44:25.429
为什么exact match还这么低align还高一些

00:44:26.730 --> 00:44:29.030
我真怀他说写我真怀他说写错了

00:44:30.429 --> 00:44:33.429
应该也没有 他可能有什么别的原因 ok

00:44:35.030 --> 00:44:36.030
就是有点奇怪是吧

00:44:38.030 --> 00:44:46.030
哦 我知道他可能exact match他可能那个东西可能很好做 他可能就是在那个矩阵里面怎么样比对就完全一样

00:44:46.530 --> 00:44:47.130
那其他的可能

00:44:47.230 --> 00:44:48.530
你就算起来更麻烦

00:44:58.260 --> 00:44:59.760
ok吧 那个下面还是什么

00:45:01.159 --> 00:45:10.860
那边别就这个这个就是他这个最后一张图的 为什么这个东西对于我们平时有啥用吗 就这个东西他做好了之后有啥用呢 大家有什么用啊对他对他来说

00:45:14.059 --> 00:45:19.860
呃 是说对我们的工作还是说这个就是这个项目本身这个项目到底意义在哪里

00:45:21.260 --> 00:45:27.860
他的意义应该就是聚合了一大堆这个数数数据库 然后说他可以把很多非常大的数据给压缩

00:45:27.860 --> 00:45:31.159
然后去做应用也还快

00:45:32.159 --> 00:45:41.659
对 嗯 然后他提供了一个在线交入式的平台 以及那个把可可以把收银下载的网址 但是他那个在线平台好像已经爆掉了 就是现在nl

00:45:45.860 --> 00:45:48.360
好 ok好 呃 世道有什么问题没有

00:45:49.860 --> 00:45:51.059
哦 我基本上都问过

00:45:51.960 --> 00:45:57.260
好 ok 那就先先暂时这样 好的 有什么问题再说 暗示呗 ok

00:45:57.860 --> 00:45:59.059
啊 是他你什么要说的吗

00:46:01.059 --> 00:46:04.059
那那老师等你待会我单独跟你说一下

00:46:05.559 --> 00:46:15.360
什么老师 待会我单独跟你说一下好的好的 ok好 要不我先退出 然后好的好的 那现在你先退吧嗯 ok ok 再见嗯
